{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1e02a-e59e-4095-ae53-f13e6ee96ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c8e03-889b-4a36-96ba-ce156aa2cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324622c3-29c6-4eae-b94d-b521f2a55961",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8bb49-5d14-425d-a85a-11daddd678d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc0415-9dbc-486f-8ae0-f1598e0132b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10813341-60ea-45d8-be93-2fbf0a81212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280cc48-46c8-478b-b6f6-cf7d35431001",
   "metadata": {},
   "source": [
    "# Load Reynolds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7d689-cbb3-45b1-8e79-8ac3241c3d86",
   "metadata": {},
   "source": [
    "## load daily soil temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa918b92-e834-4bc6-acfc-86ad8bf804be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hourly127x07soiltemperature.txt\n",
      "hourly098x97soiltemperature.txt\n",
      "hourly076x59soiltemperature.txt\n",
      "hourly057x96soiltemperature.txt\n",
      "hourly176x14soiltemperature.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "fdir = \"data/Reynolds/soiltemperature\"\n",
    "\n",
    "def loadReynolds(fname):\n",
    "    df = pd.read_csv(fname, delim_whitespace=True,comment=\"#\", encoding=\"ISO-8859-1\",on_bad_lines=\"warn\",header=None)\n",
    "    df = df[df[3]==17]\n",
    "    df_time = pd.to_datetime(df[0].astype(str) +  ' ' + df[1].astype(str) + ' ' + df[2].astype(str),format=\"%m %d %Y\")\n",
    "    \n",
    "    df2 = df[[5,6,7,8,9,10,11,12]].replace('.', np.nan).astype(float).ffill(axis=0)\n",
    "\n",
    "    df2[1] = fname.replace(fdir,\"\").replace(\"/hourly\",\"\").replace(\"soiltemperature.txt\",\"\")\n",
    "    \n",
    "    return pd.concat([df_time,df2], axis=1) \n",
    "\n",
    "soil_temp = []\n",
    "for file in os.listdir(fdir):\n",
    "    if file.endswith(\".txt\") and file.find(\"hourly\") > -1:\n",
    "        print(file)\n",
    "        path = os.path.join(fdir, file)\n",
    "        soil_temp += [loadReynolds(path)]\n",
    "\n",
    "reynolds_soil_temp = pd.concat(soil_temp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d004de7-d9e1-4212-9509-3bf90f285a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily176x14climate.txt\n",
      "daily127x07climate.txt\n",
      "daily076x59climate.txt\n"
     ]
    }
   ],
   "source": [
    "fdir2 = \"data/Reynolds\"\n",
    "\n",
    "def loadReynoldsCL(fname):\n",
    "    df = pd.read_csv(fname, delim_whitespace=True,comment=\"#\", encoding=\"ISO-8859-1\",on_bad_lines=\"warn\",header=None)\n",
    "    df = df[(df[2] > 1984)|(df[2] == 1984) & (df[0] == 12) &(df[1] > 4)]\n",
    "    df_time = pd.to_datetime(df[0].astype(str) +  ' ' + df[1].astype(str) + ' ' + df[2].astype(str),format=\"%m %d %Y\")\n",
    "    \n",
    "    df2 = df[[3,4]].replace('.', np.nan).astype(float).ffill(axis=0)\n",
    "\n",
    "    df2[1] = fname.replace(fdir2,\"\").replace(\"/daily\",\"\").replace(\"climate.txt\",\"\")\n",
    "    \n",
    "    return pd.concat([df_time,df2], axis=1) \n",
    "\n",
    "climate = []\n",
    "for file in os.listdir(fdir2):\n",
    "    if file.endswith(\".txt\") and file.find(\"daily\") > -1:\n",
    "        print(file)\n",
    "        path = os.path.join(fdir2, file)\n",
    "        climate += [loadReynoldsCL(path)]\n",
    "\n",
    "reynolds_climate = pd.concat(climate, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9aae251-610a-442a-9b1e-9e452a258382",
   "metadata": {},
   "outputs": [],
   "source": [
    "reynolds_soil_temp = reynolds_soil_temp.rename(columns={0:'DATE',1:\"LOC\",5:\"y_1\",6:\"y_2\",7:\"y_3\",8:\"y_4\",9:\"y_5\",10:\"y_6\",11:\"y_7\",12:\"y_8\",3: 20, 4: 21}) \n",
    "reynolds_climate = reynolds_climate.rename(columns={0:'DATE',1:\"LOC\"})\n",
    "climate = reynolds_climate.set_index(['DATE','LOC']).rename(columns={3: \"C1\", 4: \"C2\"}) \n",
    "soil = reynolds_soil_temp.set_index(['DATE','LOC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824cd1dd-af3a-4c24-9075-58caa38f86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reynolds_data = pd.merge(soil,climate,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5e69bb-f648-4567-87ef-537d937d4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reynolds_data.to_csv(\"all_reynolds_data.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf921e1-416b-4335-8ab5-0df3e89d570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "      <th>y_4</th>\n",
       "      <th>y_5</th>\n",
       "      <th>y_6</th>\n",
       "      <th>y_7</th>\n",
       "      <th>y_8</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th>LOC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1984-12-05</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>-2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-06</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-07</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-08</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-09</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-26</th>\n",
       "      <th>176x14</th>\n",
       "      <td>15.4</td>\n",
       "      <td>12.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-27</th>\n",
       "      <th>176x14</th>\n",
       "      <td>18.4</td>\n",
       "      <td>14.8</td>\n",
       "      <td>10.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-28</th>\n",
       "      <th>176x14</th>\n",
       "      <td>19.9</td>\n",
       "      <td>16.6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>10.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-29</th>\n",
       "      <th>176x14</th>\n",
       "      <td>20.5</td>\n",
       "      <td>17.6</td>\n",
       "      <td>13.1</td>\n",
       "      <td>12.8</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-30</th>\n",
       "      <th>176x14</th>\n",
       "      <td>20.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>13.2</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9969 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    y_1   y_2   y_3   y_4   y_5   y_6   y_7   y_8    C1    C2\n",
       "DATE       LOC                                                               \n",
       "1984-12-05 127x07   NaN   0.4   NaN   2.0   NaN   NaN   NaN   NaN  -9.8  -2.4\n",
       "1984-12-06 127x07   NaN   0.3   NaN   1.9   NaN   NaN   NaN   NaN  -9.4  -1.0\n",
       "1984-12-07 127x07   NaN   0.2   NaN   1.8   NaN   NaN   NaN   NaN  -5.2   3.6\n",
       "1984-12-08 127x07   NaN   0.1   NaN   1.7   NaN   NaN   NaN   NaN  -3.6   4.0\n",
       "1984-12-09 127x07   NaN   0.2   NaN   1.6   NaN   NaN   NaN   NaN  -0.1   5.1\n",
       "...                 ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "1996-09-26 176x14  15.4  12.5   9.5   9.6  10.2  10.4  10.8  11.0  -1.1   9.6\n",
       "1996-09-27 176x14  18.4  14.8  10.3  10.2  10.2  10.4  10.7  10.9   1.9  17.0\n",
       "1996-09-28 176x14  19.9  16.6  12.0  11.7  11.0  11.0  10.9  10.9  10.1  21.2\n",
       "1996-09-29 176x14  20.5  17.6  13.1  12.8  11.9  11.7  11.4  11.0  12.4  21.9\n",
       "1996-09-30 176x14  20.4  17.6  13.5  13.2  12.4  12.2  11.9  11.2  13.2  21.8\n",
       "\n",
       "[9969 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reynolds_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0ed8d-2138-4411-bd66-eb825c170aec",
   "metadata": {},
   "source": [
    "# UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52ed1ef-e310-4ba8-8404-c70579c2af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosmos-uk_alic1_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_tadhm_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_crich_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_lizrd_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_morly_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_redhl_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_sourh_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_plynl_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_wrttl_hydrosoil_daily_2017-2022.csv\n",
      "cosmos-uk_sydlg_hydrosoil_daily_2018-2022.csv\n",
      "cosmos-uk_glens_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_holln_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_hartw_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_hillb_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_bickl_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_cardt_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_moorh_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_eustn_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_eastb_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_riseh_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_hybry_hydrosoil_daily_2017-2022.csv\n",
      "cosmos-uk_rothd_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_balrd_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_wyth1_hydrosoil_daily_2013-2022.csv\n",
      "cosmos-uk_nwyke_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_fivet_hydrosoil_daily_2018-2022.csv\n",
      "cosmos-uk_chobh_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_glenw_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_stght_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_lulln_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_hadlw_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_harwd_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_henfs_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_hlacy_hydrosoil_daily_2018-2022.csv\n",
      "cosmos-uk_cgarw_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_sheep_hydrosoil_daily_2013-2022.csv\n",
      "cosmos-uk_finch_hydrosoil_daily_2017-2022.csv\n",
      "cosmos-uk_gisbn_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_bunny_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_elmst_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_lodtn_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_waddn_hydrosoil_daily_2013-2022.csv\n",
      "cosmos-uk_chimn_hydrosoil_daily_2013-2022.csv\n",
      "cosmos-uk_wimpl_hydrosoil_daily_2019-2022.csv\n",
      "cosmos-uk_portn_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_spenf_hydrosoil_daily_2016-2022.csv\n",
      "cosmos-uk_morem_hydrosoil_daily_2018-2022.csv\n",
      "cosmos-uk_cochn_hydrosoil_daily_2017-2022.csv\n",
      "cosmos-uk_rdmer_hydrosoil_daily_2015-2022.csv\n",
      "cosmos-uk_coclp_hydrosoil_daily_2014-2022.csv\n",
      "cosmos-uk_stips_hydrosoil_daily_2014-2022.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LWIN</th>\n",
       "      <th>LWOUT</th>\n",
       "      <th>SWIN</th>\n",
       "      <th>SWOUT</th>\n",
       "      <th>RN</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>PRECIP_TIPPING</th>\n",
       "      <th>PRECIP_RAINE</th>\n",
       "      <th>PA</th>\n",
       "      <th>TA</th>\n",
       "      <th>...</th>\n",
       "      <th>STP_TSOIL50</th>\n",
       "      <th>COSMOS_VWC</th>\n",
       "      <th>CTS_MOD_CORR</th>\n",
       "      <th>D86_75M</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNOW_DEPTH</th>\n",
       "      <th>SWE</th>\n",
       "      <th>ALBEDO</th>\n",
       "      <th>PE</th>\n",
       "      <th>GCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th>LOC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-03-06</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-07</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.7</td>\n",
       "      <td>30.5</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>40.8</td>\n",
       "      <td>1129.96052</td>\n",
       "      <td>22.50153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.126</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-08</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>27.1</td>\n",
       "      <td>30.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1010.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7</td>\n",
       "      <td>42.9</td>\n",
       "      <td>1120.16381</td>\n",
       "      <td>22.02295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-09</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>28.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1011.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>38.8</td>\n",
       "      <td>1140.46891</td>\n",
       "      <td>22.99462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-10</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.5</td>\n",
       "      <td>29.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1016.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>45.9</td>\n",
       "      <td>1106.90799</td>\n",
       "      <td>21.40022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>28.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>962.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>29.1</td>\n",
       "      <td>29.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>945.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>26.3</td>\n",
       "      <td>28.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>27.1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>941.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>30.1</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>945.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135654 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  LWIN  LWOUT  SWIN  SWOUT   RN  PRECIP  PRECIP_TIPPING  \\\n",
       "DATE       LOC                                                            \n",
       "2015-03-06 ALIC1   NaN    NaN   NaN    NaN  NaN     NaN             NaN   \n",
       "2015-03-07 ALIC1  23.7   30.5  13.3    1.9  4.6     0.0             NaN   \n",
       "2015-03-08 ALIC1  27.1   30.5   4.8    0.7  0.7     0.0             NaN   \n",
       "2015-03-09 ALIC1  28.3   29.7   2.4    0.4  0.7     0.1             NaN   \n",
       "2015-03-10 ALIC1  23.5   29.8  12.4    1.8  4.3     0.0             NaN   \n",
       "...                ...    ...   ...    ...  ...     ...             ...   \n",
       "2022-12-27 STIPS  28.4   29.0   0.4    0.1 -0.3     2.9             NaN   \n",
       "2022-12-28 STIPS  29.1   29.8   0.6    0.1 -0.2     6.4             NaN   \n",
       "2022-12-29 STIPS  26.3   28.7   1.7    0.4 -1.1     2.5             NaN   \n",
       "2022-12-30 STIPS  27.1   29.6   1.2    0.2 -1.5     2.0             NaN   \n",
       "2022-12-31 STIPS  30.1   30.4   0.7    0.1  0.3    21.6             NaN   \n",
       "\n",
       "                  PRECIP_RAINE      PA   TA  ...  STP_TSOIL50  COSMOS_VWC  \\\n",
       "DATE       LOC                               ...                            \n",
       "2015-03-06 ALIC1           NaN     NaN  NaN  ...          NaN         NaN   \n",
       "2015-03-07 ALIC1           NaN  1012.2  8.6  ...          5.6        40.8   \n",
       "2015-03-08 ALIC1           NaN  1010.3  8.4  ...          5.7        42.9   \n",
       "2015-03-09 ALIC1           NaN  1011.8  6.2  ...          5.9        38.8   \n",
       "2015-03-10 ALIC1           NaN  1016.6  7.2  ...          5.9        45.9   \n",
       "...                        ...     ...  ...  ...          ...         ...   \n",
       "2022-12-27 STIPS           NaN   962.5  4.7  ...          6.3        52.5   \n",
       "2022-12-28 STIPS           NaN   945.7  6.4  ...          6.2        52.5   \n",
       "2022-12-29 STIPS           NaN   946.7  4.1  ...          6.3        52.5   \n",
       "2022-12-30 STIPS           NaN   941.3  6.2  ...          6.3        52.5   \n",
       "2022-12-31 STIPS           NaN   945.3  7.6  ...          6.4        52.5   \n",
       "\n",
       "                  CTS_MOD_CORR   D86_75M  SNOW  SNOW_DEPTH      SWE  ALBEDO  \\\n",
       "DATE       LOC                                                                \n",
       "2015-03-06 ALIC1           NaN       NaN   NaN         NaN      NaN     NaN   \n",
       "2015-03-07 ALIC1    1129.96052  22.50153   0.0         NaN      NaN   0.126   \n",
       "2015-03-08 ALIC1    1120.16381  22.02295   0.0         NaN      NaN   0.116   \n",
       "2015-03-09 ALIC1    1140.46891  22.99462   0.0         NaN      NaN   0.111   \n",
       "2015-03-10 ALIC1    1106.90799  21.40022   0.0         NaN      NaN   0.121   \n",
       "...                        ...       ...   ...         ...      ...     ...   \n",
       "2022-12-27 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.139   \n",
       "2022-12-28 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.138   \n",
       "2022-12-29 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.189   \n",
       "2022-12-30 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.152   \n",
       "2022-12-31 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.161   \n",
       "\n",
       "                   PE    GCC  \n",
       "DATE       LOC                \n",
       "2015-03-06 ALIC1  NaN    NaN  \n",
       "2015-03-07 ALIC1  1.5    NaN  \n",
       "2015-03-08 ALIC1  0.4    NaN  \n",
       "2015-03-09 ALIC1  0.2    NaN  \n",
       "2015-03-10 ALIC1  1.4    NaN  \n",
       "...               ...    ...  \n",
       "2022-12-27 STIPS  0.1  0.335  \n",
       "2022-12-28 STIPS  0.3  0.334  \n",
       "2022-12-29 STIPS  0.4  0.334  \n",
       "2022-12-30 STIPS  0.4  0.334  \n",
       "2022-12-31 STIPS  0.2  0.336  \n",
       "\n",
       "[135654 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_data = \"data/UK/catalogue.ceh.ac.uk/datastore/eidchub/1/\"\n",
    "climate_soil = []\n",
    "\n",
    "def loadUK(fname):\n",
    "    df = pd.read_csv(fname, sep=\",\",comment=\"#\", on_bad_lines=\"warn\")\n",
    "\n",
    "    df1 = df.iloc[:, :2]\n",
    "    df2 = df.iloc[:, 2:]    \n",
    "    df2 = df2.astype(float).replace(-9999.0, np.nan).ffill(axis=0)    \n",
    "    \n",
    "    return pd.concat([df1,df2], axis=1)\n",
    "    \n",
    "\n",
    "for file in os.listdir(uk_data):\n",
    "    if file.endswith(\".csv\") and file.find(\"daily\") > -1 and file.find(\"flags\") == -1 and file.find(\"metadata\") == -1:\n",
    "        print(file)\n",
    "        path = os.path.join(uk_data, file)\n",
    "        climate_soil += [loadUK(path)]\n",
    "\n",
    "climate_soil =  pd.concat(climate_soil, axis=0)\n",
    "climate_soil = climate_soil.rename(columns={\"DATE_TIME\":\"DATE\",\"SITE_ID\":\"LOC\",\"TDT1_TSOIL\":\"y_1\",\"TDT2_TSOIL\":\"y_2\",\"TDT3_TSOIL\":\"y_3\",\"TDT4_TSOIL\":\"y_4\",\"TDT5_TSOIL\":\"y_5\",\"TDT6_TSOIL\":\"y_6\",\"TDT7_TSOIL\":\"y_7\",\"TDT8_TSOIL\":\"y_8\",\"TDT9_TSOIL\":\"y_9\",\"TDT10_TSOIL\":\"y_10\"}) \n",
    "climate_soil['DATE']= pd.to_datetime(climate_soil['DATE'])\n",
    "climate_soil.set_index([\"DATE\",\"LOC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82e6c7a-5695-479e-aec3-68498a28b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil.rename(columns={\"DATE_TIME\":\"DATE\",\"SITE_ID\":\"LOC\",\"TDT1_TSOIL\":\"y_1\",\"TDT2_TSOIL\":\"y_2\",\"TDT3_TSOIL\":\"y_3\",\"TDT4_TSOIL\":\"y_4\",\"TDT5_TSOIL\":\"y_5\",\"TDT6_TSOIL\":\"y_6\",\"TDT7_TSOIL\":\"y_7\",\"TDT8_TSOIL\":\"y_8\",\"TDT9_TSOIL\":\"y_9\",\"TDT10_TSOIL\":\"y_10\"}) \n",
    "climate_soil = climate_soil.set_index([\"DATE\",\"LOC\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1672478-1885-4806-910f-e80cb44ac9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil.to_csv(\"uk_soil.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36080b6-8644-4a5f-860d-f155803dc18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LWIN</th>\n",
       "      <th>LWOUT</th>\n",
       "      <th>SWIN</th>\n",
       "      <th>SWOUT</th>\n",
       "      <th>RN</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>PRECIP_TIPPING</th>\n",
       "      <th>PRECIP_RAINE</th>\n",
       "      <th>PA</th>\n",
       "      <th>TA</th>\n",
       "      <th>...</th>\n",
       "      <th>STP_TSOIL50</th>\n",
       "      <th>COSMOS_VWC</th>\n",
       "      <th>CTS_MOD_CORR</th>\n",
       "      <th>D86_75M</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNOW_DEPTH</th>\n",
       "      <th>SWE</th>\n",
       "      <th>ALBEDO</th>\n",
       "      <th>PE</th>\n",
       "      <th>GCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th>LOC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-03-06</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-07</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.7</td>\n",
       "      <td>30.5</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>40.8</td>\n",
       "      <td>1129.96052</td>\n",
       "      <td>22.50153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.126</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-08</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>27.1</td>\n",
       "      <td>30.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1010.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7</td>\n",
       "      <td>42.9</td>\n",
       "      <td>1120.16381</td>\n",
       "      <td>22.02295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-09</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>28.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1011.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>38.8</td>\n",
       "      <td>1140.46891</td>\n",
       "      <td>22.99462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-10</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.5</td>\n",
       "      <td>29.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1016.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>45.9</td>\n",
       "      <td>1106.90799</td>\n",
       "      <td>21.40022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>28.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>962.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>29.1</td>\n",
       "      <td>29.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>945.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>26.3</td>\n",
       "      <td>28.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>27.1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>941.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <th>STIPS</th>\n",
       "      <td>30.1</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>945.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>52.5</td>\n",
       "      <td>1317.61816</td>\n",
       "      <td>32.53471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.56238</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135654 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  LWIN  LWOUT  SWIN  SWOUT   RN  PRECIP  PRECIP_TIPPING  \\\n",
       "DATE       LOC                                                            \n",
       "2015-03-06 ALIC1   NaN    NaN   NaN    NaN  NaN     NaN             NaN   \n",
       "2015-03-07 ALIC1  23.7   30.5  13.3    1.9  4.6     0.0             NaN   \n",
       "2015-03-08 ALIC1  27.1   30.5   4.8    0.7  0.7     0.0             NaN   \n",
       "2015-03-09 ALIC1  28.3   29.7   2.4    0.4  0.7     0.1             NaN   \n",
       "2015-03-10 ALIC1  23.5   29.8  12.4    1.8  4.3     0.0             NaN   \n",
       "...                ...    ...   ...    ...  ...     ...             ...   \n",
       "2022-12-27 STIPS  28.4   29.0   0.4    0.1 -0.3     2.9             NaN   \n",
       "2022-12-28 STIPS  29.1   29.8   0.6    0.1 -0.2     6.4             NaN   \n",
       "2022-12-29 STIPS  26.3   28.7   1.7    0.4 -1.1     2.5             NaN   \n",
       "2022-12-30 STIPS  27.1   29.6   1.2    0.2 -1.5     2.0             NaN   \n",
       "2022-12-31 STIPS  30.1   30.4   0.7    0.1  0.3    21.6             NaN   \n",
       "\n",
       "                  PRECIP_RAINE      PA   TA  ...  STP_TSOIL50  COSMOS_VWC  \\\n",
       "DATE       LOC                               ...                            \n",
       "2015-03-06 ALIC1           NaN     NaN  NaN  ...          NaN         NaN   \n",
       "2015-03-07 ALIC1           NaN  1012.2  8.6  ...          5.6        40.8   \n",
       "2015-03-08 ALIC1           NaN  1010.3  8.4  ...          5.7        42.9   \n",
       "2015-03-09 ALIC1           NaN  1011.8  6.2  ...          5.9        38.8   \n",
       "2015-03-10 ALIC1           NaN  1016.6  7.2  ...          5.9        45.9   \n",
       "...                        ...     ...  ...  ...          ...         ...   \n",
       "2022-12-27 STIPS           NaN   962.5  4.7  ...          6.3        52.5   \n",
       "2022-12-28 STIPS           NaN   945.7  6.4  ...          6.2        52.5   \n",
       "2022-12-29 STIPS           NaN   946.7  4.1  ...          6.3        52.5   \n",
       "2022-12-30 STIPS           NaN   941.3  6.2  ...          6.3        52.5   \n",
       "2022-12-31 STIPS           NaN   945.3  7.6  ...          6.4        52.5   \n",
       "\n",
       "                  CTS_MOD_CORR   D86_75M  SNOW  SNOW_DEPTH      SWE  ALBEDO  \\\n",
       "DATE       LOC                                                                \n",
       "2015-03-06 ALIC1           NaN       NaN   NaN         NaN      NaN     NaN   \n",
       "2015-03-07 ALIC1    1129.96052  22.50153   0.0         NaN      NaN   0.126   \n",
       "2015-03-08 ALIC1    1120.16381  22.02295   0.0         NaN      NaN   0.116   \n",
       "2015-03-09 ALIC1    1140.46891  22.99462   0.0         NaN      NaN   0.111   \n",
       "2015-03-10 ALIC1    1106.90799  21.40022   0.0         NaN      NaN   0.121   \n",
       "...                        ...       ...   ...         ...      ...     ...   \n",
       "2022-12-27 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.139   \n",
       "2022-12-28 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.138   \n",
       "2022-12-29 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.189   \n",
       "2022-12-30 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.152   \n",
       "2022-12-31 STIPS    1317.61816  32.53471   0.0         NaN  5.56238   0.161   \n",
       "\n",
       "                   PE    GCC  \n",
       "DATE       LOC                \n",
       "2015-03-06 ALIC1  NaN    NaN  \n",
       "2015-03-07 ALIC1  1.5    NaN  \n",
       "2015-03-08 ALIC1  0.4    NaN  \n",
       "2015-03-09 ALIC1  0.2    NaN  \n",
       "2015-03-10 ALIC1  1.4    NaN  \n",
       "...               ...    ...  \n",
       "2022-12-27 STIPS  0.1  0.335  \n",
       "2022-12-28 STIPS  0.3  0.334  \n",
       "2022-12-29 STIPS  0.4  0.334  \n",
       "2022-12-30 STIPS  0.4  0.334  \n",
       "2022-12-31 STIPS  0.2  0.336  \n",
       "\n",
       "[135654 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4eaa79-83d9-4c50-87ff-6f8e664ecd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil = climate_soil.drop([\"SNOW_DEPTH\", \"TDT1_VWC\",\"TDT2_VWC\",\"TDT3_VWC\",\"TDT4_VWC\",\"TDT5_VWC\",\"TDT6_VWC\",\"TDT7_VWC\",\"TDT8_VWC\",\"TDT9_VWC\",\"TDT10_VWC\",\"PRECIP_TIPPING\",\"PRECIP_RAINE\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b260c2-cee3-4d1e-bc45-10db2624173d",
   "metadata": {},
   "source": [
    "# Preprocessing of the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf6c33-e6d6-45b7-9da9-7f8d4c1697c0",
   "metadata": {},
   "source": [
    "### Split onto 28 day long fragments. Remove fragments, which do not containt more than 30% of target values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff8e5b-2621-4e73-9b50-6490ac384923",
   "metadata": {},
   "source": [
    "### We do not consider temperatures for other soil layers as fatures deliberatly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6384146e-1ecc-4b83-b779-7029e3d4b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "PERIOD = 28\n",
    "TRHX = 0.3\n",
    "\n",
    "def daterange(start_date, end_date, step):\n",
    "    for n in range(0,int((end_date - start_date).days), step):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "all_data = []\n",
    "for df in [all_reynolds_data, climate_soil]:\n",
    "    locations = df.index.get_level_values(1)\n",
    "    \n",
    "    rdatasX = {}\n",
    "    rdatasY = {}\n",
    "    for l in set(locations):\n",
    "        ld = df.query(\"LOC == '\" + l + \"'\").reset_index().set_index(\"DATE\").drop(\"LOC\",axis=1)\n",
    "        all_start = ld.index.min()\n",
    "        all_end = ld.index.max()\n",
    "    \n",
    "        for start in daterange(all_start, all_end, PERIOD):\n",
    "            end = start + timedelta(PERIOD)\n",
    "            period_data = ld.loc[start:end]\n",
    "            \n",
    "            y_columns = set([c for c in df.columns if c.find(\"y_\") > -1])\n",
    "            not_y_columns = [c for c in df.columns if c.find(\"y_\") == -1]\n",
    "            for y_counter in y_columns:\n",
    "                if y_counter not in rdatasX: \n",
    "                    rdatasX[y_counter] = []\n",
    "                    rdatasY[y_counter] = []\n",
    "                    \n",
    "                y_ = period_data[y_counter].to_numpy().astype(float)\n",
    "                y = y_[1:]\n",
    "    \n",
    "                nans = np.count_nonzero(np.isnan(y))\n",
    "    \n",
    "                if float(nans) / y.shape[0] < TRHX and y.shape[0] == PERIOD:\n",
    "                    X = period_data[not_y_columns].to_numpy()[:-1].astype(float)\n",
    "                    X = np.hstack([X,y_[:-1].reshape(-1,1)]) # add current y as a feature\n",
    "                    old_dim = X.shape[1]\n",
    "                    X = imp.fit_transform(X)\n",
    "                    if X.shape[1] == old_dim:\n",
    "                        nans = np.count_nonzero(np.isnan(X))\n",
    "                        if nans == 0:\n",
    "                            y = imp.fit_transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "                            rdatasX[y_counter].append(X)\n",
    "                            rdatasY[y_counter].append(y)\n",
    "                    \n",
    "    all_data.append({k:[np.asarray(rdatasX[k]), np.asarray(rdatasY[k])] for k in rdatasX})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "455aaf36-c9e2-4c5c-a864-c5123484fe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['y_2', 'y_7', 'y_8', 'y_5', 'y_6', 'y_1', 'y_4', 'y_3'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f70c021-80be-439d-9a0e-5f72fe081e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "for k in all_data[0]:\n",
    "    all_data[0][k][0] = normalize(all_data[0][k][0].reshape(-1,all_data[0][k][0].shape[2]),axis=0).reshape(all_data[0][k][0].shape)\n",
    "    \n",
    "for k in all_data[1]:\n",
    "    all_data[1][k][0] = normalize(all_data[1][k][0].reshape(-1,all_data[1][k][0].shape[2]),axis=0).reshape(all_data[1][k][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205d89a-3a6a-4f01-9537-2d70b3cc3018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a59a529a-614a-49e0-884b-0ab0ae3c6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dict_data = {}\n",
    "dict_data[\"Reynolds\"] = {}\n",
    "dict_data[\"UK\"] = {}\n",
    "\n",
    "for k in all_data[0]:\n",
    "    x01,x02,y01,y02 = train_test_split(all_data[0][k][0], all_data[0][k][1], test_size=0.3,random_state=42)\n",
    "    dict_data[\"Reynolds\"][k] = {\"train\":{\"X\":x01,\"y\":y01},\"test\":{\"X\":x02,\"y\":y02}}\n",
    "    \n",
    "for k in all_data[1]:\n",
    "    x11,x12,y11,y12 = train_test_split(all_data[1][k][0], all_data[1][k][1],test_size=0.3,random_state=42)\n",
    "    dict_data[\"UK\"][k] = {\"train\":{\"X\":x11,\"y\":y11},\"test\":{\"X\":x12,\"y\":y12}} \n",
    "\n",
    "all_data = dict_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220b854-2101-4a5b-8d3a-58256166190b",
   "metadata": {},
   "source": [
    "# Find hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149bcd0-33fb-4882-bc91-7e69feb34808",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ac0b2-b494-4f11-9966-6988ed981e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==4.21.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37d7c5-c860-4144-87be-25432e02a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cab2f4-eef5-4c80-a528-bbe0fa4672d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801dd1a7-1dc2-454b-bf0c-afce7e161d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50015bf6-285c-4b21-91e2-cb3c9ebe7a2c",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ee698-ed63-437e-8051-e9152270dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import optuna\n",
    "from tkan import TKAN, BSplineActivation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "nn_data = []\n",
    "\n",
    "\n",
    "def make_modelLSTM(input_shape, hidden_size, dropout):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "    #dim = tf.zeros([batch_size,hidden_size])  \n",
    "    output_layer = tf.keras.layers.LSTM(hidden_size, return_sequences=True,dropout=dropout)(input_layer)#, initial_state=[dim, dim])\n",
    "    output_layer2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear'))(output_layer)    \n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer2)\n",
    " \n",
    "def make_GRU(input_shape, hidden_size, dropout):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "    output_layer = tf.keras.layers.GRU(hidden_size, return_sequences=True,dropout=dropout)(input_layer)#, initial_state=[dim, dim])\n",
    "    output_layer2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear'))(output_layer)    \n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "\n",
    "def make_modelTKAN(input_shape, hidden_size, dropout):\n",
    "    model = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "          TKAN(hidden_size, tkan_activations=[BSplineActivation(3)], return_sequences=True, use_bias=True, kernel_regularizer=regularizers.L2(dropout),\n",
    "    bias_regularizer=regularizers.L2(dropout), recurrent_regularizer=regularizers.L2(dropout)),\n",
    "          tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear')),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#models = {\"LSTM\":make_modelLSTM, \"GRU\": make_GRU, \"TKAN\": make_modelTKAN}\n",
    "models = {\"TKAN\": make_modelTKAN}\n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    #for ds_name in all_data:\n",
    "    for ds_name in [\"UK\"]:    \n",
    "        #for depth in all_data[ds_name]:\n",
    "        for depth in ['y_9']:    \n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"]\n",
    "            Y_test = dat[\"test\"][\"y\"]\n",
    "            batch_size = 4       \n",
    "    \n",
    "            def objective(trial):\n",
    "                lr = trial.suggest_float('lr', 0.0001, 0.1)\n",
    "                hidden_size = trial.suggest_int('hs', 2, 8)\n",
    "                en = trial.suggest_int('en', 10, 500)\n",
    "\n",
    "                if model_name == \"TKAN\":\n",
    "                    do = trial.suggest_float('dropout', 1e-5, 1e-2)\n",
    "                else:    \n",
    "                    do = trial.suggest_float('dropout', 0.05, 0.2)\n",
    "                    \n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(input_shape=x_train.shape[1:],hidden_size=hidden_size, dropout = do)\n",
    "                # \n",
    "                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                \n",
    "                    model.compile(\n",
    "                         optimizer=opt,\n",
    "                         loss=\"mean_squared_error\",\n",
    "                         metrics=[\"mean_squared_error\"],\n",
    "                    )\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=en,\n",
    "                         verbose=0,\n",
    "                    )\n",
    "                    try:\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                    except:\n",
    "                        scores.append(500)\n",
    "                return np.asarray(scores).mean() \n",
    "                \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=20)    \n",
    "            \n",
    "            lr = study.best_trial.params[\"lr\"]     \n",
    "            hs = study.best_trial.params[\"hs\"]     \n",
    "            en = study.best_trial.params[\"en\"]   \n",
    "            do = study.best_trial.params[\"dropout\"]   \n",
    "    \n",
    "            model = make_model(input_shape=x_train.shape[1:],hidden_size=hs, dropout = do)\n",
    "        # \n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "            model.compile(\n",
    "                 optimizer=opt,\n",
    "                 loss=\"mean_squared_error\",\n",
    "                 metrics=[\"mean_squared_error\"],\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=en,\n",
    "                 verbose=0,\n",
    "            )        \n",
    "            try:\n",
    "                y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "                mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "                mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "                print(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max())     \n",
    "                nn_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "            except:\n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cc727-6ec0-4692-9292-dc887ea20006",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0c1e4-e919-4263-a024-313b3fc17b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install deep-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e55aa-3f02-4947-8fb0-0afbd49315e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cb9f5-d07a-40da-a96c-fc6ec26e8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4964ea9-4579-4ac4-881d-9f03c028045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Reynolds\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875b738-00c9-466d-acb9-02b3cb7c3b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-07 10:41:19,496] A new study created in memory with name: no-name-2e54c22d-2a0c-4581-a6a9-f709245e50d8\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelXGB(max_depth,layers,C):\n",
    "    return xgb.XGBRegressor(max_depth = max_depth, n_estimators = layers)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C):\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4)\n",
    "\n",
    "def make_modelBoosted(max_depth,layers,C):\n",
    "    return CascadeBoostingRegressor(C=C, n_layers=layers, n_estimators = 4, max_depth=max_depth, n_iter_no_change = 1, validation_fraction = 0.1, learning_rate = 0.9)\n",
    "\n",
    "\n",
    "#models = {\"XGB\":make_modelXGB,\"Cascade Forest\":make_modelCascade, \"Boosted Forest\": make_modelBoosted}\n",
    "models = {\"Boosted Forest\": make_modelBoosted}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in [\"Reynolds\"]:\n",
    "        for depth in ['y_7', 'y_6', 'y_5']:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])\n",
    "            x_test = dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2])\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 3)\n",
    "\n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 1, 2000)\n",
    "                else:\n",
    "                    C = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(max_depth,layers,C)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=20)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]  \n",
    "            else:\n",
    "                C = 0\n",
    "            model = make_model(max_depth,layers,C)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            print(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max())     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9f557-8d20-4348-9bbc-c405b089c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869dd8a-6ea3-4538-b81f-0ae41b97d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy==1.22.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3441aa5-1ed2-4a2d-a015-ce7ba440e584",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517ca83-e93c-430f-b746-3267b2c14792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "rf_data = [] \n",
    "\n",
    "for ds_name in all_data:\n",
    "    for depth in all_data[ds_name]:\n",
    "        best = 1000\n",
    "        best_d = 2\n",
    "        for n_estimators in [100]:\n",
    "            for tdepth in [2,3,5,7,10,12]:\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = RandomForestRegressor(n_estimators = n_estimators, max_depth = tdepth)\n",
    "                    dat = all_data[ds_name][depth]\n",
    "                    model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])[train_index], dat[\"train\"][\"y\"].flatten()[train_index])\n",
    "                    y_pred = model.predict(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]))[test_index])\n",
    "                    scores.append(mean_squared_error(y_pred, dat[\"train\"][\"y\"].flatten())[test_index])\n",
    "                    \n",
    "                if np.asarray(scores).mean() < best:\n",
    "                    best = np.asarray(scores).mean()\n",
    "                    best_d = tdepth\n",
    "\n",
    "        model = RandomForestRegressor(n_estimators = n_estimators, max_depth = best_d)\n",
    "        dat = all_data[ds_name][depth]\n",
    "        model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]), dat[\"train\"][\"y\"].flatten())\n",
    "        y_pred = model.predict(dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2]))\n",
    "        mse_score = mean_squared_error(y_pred, dat[\"test\"][\"y\"].flatten())\n",
    "        mae_score = mean_absolute_error(y_pred, dat[\"test\"][\"y\"].flatten())        \n",
    "                \n",
    "        print(\"Random Forest\", ds_name,depth,mse_score, mae_score, dat[\"test\"][\"y\"].min(),dat[\"test\"][\"y\"].max())\n",
    "        rf_data.append([\"Random Forest\", ds_name,depth,mse_score, mae_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449b898-9b7d-4e3d-a746-f90482946d41",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec049036-9f0b-4688-afe5-dff78ceadc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for ds_name in [\"UK\"]: #all_data:\n",
    "    for depth in ['y_5', 'y_1', 'y_10', 'y_7', 'y_8', 'y_9', 'y_6']:#all_data[ds_name]:\n",
    "        model = CascadeBoostingRegressor(C=1000, n_layers=10, verbose=2, n_estimators = 4, max_depth=1, n_iter_no_change = 1, validation_fraction = 0.1, learning_rate = 0.9)\n",
    "        dat = all_data[ds_name][depth]\n",
    "        model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]), dat[\"train\"][\"y\"].flatten())\n",
    "        y_pred = model.predict(dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2]))\n",
    "        mse_score = mean_squared_error(y_pred, dat[\"test\"][\"y\"].flatten())\n",
    "        mae_score = mean_absolute_error(y_pred, dat[\"test\"][\"y\"].flatten())\n",
    "        print(ds_name,depth,mse_score, mae_score, dat[\"test\"][\"y\"].min(),dat[\"test\"][\"y\"].max())\n",
    "        del model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3af238-8c80-4c77-8df5-f6eac4206b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reynolds y_3 1.196124186330187 0.7492197406658464 -8.6 31.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fb083-ca41-44ba-8f49-312a7d3993b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867463c0-a81a-4b7d-81e0-529201642427",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e1f56-5a24-428c-a946-b71f184fafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b2a3e-f914-406e-a805-68a212694335",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in all_data:\n",
    "    for k in all_data[ds]:\n",
    "        print(all_data[ds][k]['train']['X'].shape,all_data[ds][k]['test']['X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7964d-5b3b-41c1-9d93-861a3f06a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "195 / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda29e86-b147-4100-abb9-89d5dcc9f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffa4d2-d493-4fe5-93c3-2e1de5afaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for ds_name in all_data:\n",
    "    for depth in all_data[ds_name]:\n",
    "        for n_estimators in [10, 30, 100]:\n",
    "            for tdepth in [2,3,5,7,10,12]:\n",
    "                model = RandomForestRegressor(n_estimators = n_estimators, max_depth = tdepth)\n",
    "                dat = all_data[ds_name][depth]\n",
    "                model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]), dat[\"train\"][\"y\"].flatten())\n",
    "                y_pred = model.predict(dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2]))\n",
    "                mse_score = mean_squared_error(y_pred, dat[\"test\"][\"y\"].flatten())\n",
    "                mae_score = mean_absolute_error(y_pred, dat[\"test\"][\"y\"].flatten())\n",
    "                \n",
    "                print(n_estimators, tdepth, ds_name,depth,mse_score, mae_score, dat[\"test\"][\"y\"].min(),dat[\"test\"][\"y\"].max())\n",
    "                del model\n",
    "        break\n",
    "    break   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e13640-b055-4edc-8b33-93fba942510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"UK\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75468b43-5793-41b2-b2fe-8ae15cd47c19",
   "metadata": {},
   "source": [
    "# TKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f37f1-05bd-452e-b13f-76c4ecc0141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkan import TKAN, BSplineActivation\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "def make_model(input_shape, hidden_size):\n",
    "    model = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "          TKAN(hidden_size, tkan_activations=[BSplineActivation(3)], return_sequences=True, use_bias=True, kernel_regularizer=regularizers.L2(1e-4),\n",
    "    bias_regularizer=regularizers.L2(1e-4), recurrent_regularizer=regularizers.L2(1e-4)),\n",
    "          tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear')),\n",
    "    ])\n",
    "    return model\n",
    "# \n",
    "\n",
    "#def objective(trial):\n",
    "\n",
    "for ds_name in [\"UK\"]:\n",
    "    for depth in all_data[ds_name]:\n",
    "        dat = all_data[ds_name][depth]\n",
    "        x_train = dat[\"train\"][\"X\"]\n",
    "        x_test = dat[\"test\"][\"X\"]\n",
    "        Y_train = dat[\"train\"][\"y\"]\n",
    "        Y_test = dat[\"test\"][\"y\"]\n",
    "        batch_size = 4       \n",
    "        model = make_model(input_shape=x_train.shape[1:],hidden_size=8)\n",
    "        # \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "        \n",
    "        model.compile(\n",
    "             optimizer=opt,\n",
    "             loss=\"mean_squared_error\",\n",
    "             metrics=[\"mean_squared_error\"],\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "             x_train,\n",
    "             Y_train,\n",
    "             batch_size=batch_size,\n",
    "             epochs=epochs,\n",
    "             verbose=0,\n",
    "        )\n",
    "        # \n",
    "        #batch_size = x_test.shape[0]\n",
    "        y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "\n",
    "        try:\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            print(ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max())     \n",
    "        except:\n",
    "            print(ds_name,depth,\" test error!\")\n",
    "        \n",
    "        y_pred = model.predict(x_train) #, batch_size=batch_size)\n",
    "        try:\n",
    "            mse_score = mean_squared_error(Y_train.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_train.flatten(),y_pred.flatten())\n",
    "            print(\"train: \", ds_name,depth,mse_score, mae_score, Y_train.min(),Y_train.max()) \n",
    "        except:\n",
    "            print(ds_name,depth,\" train error!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
