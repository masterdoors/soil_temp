{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba409a4f-6e35-4614-818f-f2272c3c157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7f3b2-7f98-424e-bc42-9480cc40f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830acedf-3cf2-4fbd-ac93-3a43c95ae8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebf8c2-5dbe-46d4-b1a1-29f0588b18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "def printf(*args, fname=\"log.txt\"):\n",
    "    with open(join(\"test_outputs\",fname),\"a+\") as f:\n",
    "        for a in args:\n",
    "            f.write(str(a) + \" \")\n",
    "        f.write(\"\\n\") \n",
    "    print(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315515b-ae62-4adf-a08f-f08f76e46be0",
   "metadata": {},
   "source": [
    "# KDD 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424e065-2578-404c-b9e5-0ef4f71beaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p lifetime-value/kdd_cup_98\n",
    "#!wget https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98lrn.zip -P lifetime-value/kdd_cup_98/\n",
    "#!wget https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98val.zip -P lifetime-value/kdd_cup_98/\n",
    "#!wget https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/valtargt.txt -P lifetime-value/kdd_cup_98/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcdebe-71d6-469b-a1bc-9e80ea80df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip lifetime-value/kdd_cup_98/cup98lrn.zip\n",
    "#!unzip lifetime-value/kdd_cup_98/cup98val.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef8981-341c-4e79-9751-14deac11a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv('cup98LRN.txt')\n",
    "num_train = df_train.shape[0]\n",
    "df_eval = pd.read_csv('cup98VAL.txt')\n",
    "df_eval_target = pd.read_csv('lifetime-value/kdd_cup_98/valtargt.txt')\n",
    "df_eval = df_eval.merge(df_eval_target, on='CONTROLN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6410da4-5ce8-4e55-9b24-333082e182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original: https://github.com/google/lifetime_value/blob/master/notebooks/kdd_cup_98/regression.ipynb\n",
    "df = pd.concat([df_train, df_eval], axis=0, sort=True)\n",
    "y = df['TARGET_D'][:num_train]\n",
    "\n",
    "VOCAB_FEATURES = [\n",
    "    'ODATEDW',  # date of donor's first gift (YYMM)\n",
    "    'OSOURCE',  # donor acquisition mailing list\n",
    "    'TCODE',    # donor title code\n",
    "    'STATE',\n",
    "    'ZIP',\n",
    "    'DOMAIN',   # urbanicity level and socio-economic status of the neighborhood\n",
    "    'CLUSTER',  # socio-economic status\n",
    "    'GENDER',\n",
    "    'MAXADATE', # date of the most recent promotion received\n",
    "    'MINRDATE',\n",
    "    'LASTDATE',\n",
    "    'FISTDATE',\n",
    "    'RFA_2A',\n",
    "]\n",
    "\n",
    "df['ODATEDW'] = df['ODATEDW'].astype('str')\n",
    "df['TCODE'] = df['TCODE'].apply(\n",
    "    lambda x: '{:03d}'.format(x // 1000 if x > 1000 else x))\n",
    "df['ZIP'] = df['ZIP'].str.slice(0, 5)\n",
    "df['MAXADATE'] = df['MAXADATE'].astype('str')\n",
    "df['MINRDATE'] = df['MINRDATE'].astype('str')\n",
    "df['LASTDATE'] = df['LASTDATE'].astype('str')\n",
    "df['FISTDATE'] = df['FISTDATE'].astype('str')\n",
    "\n",
    "\n",
    "def label_encoding(y, frequency_threshold=100):\n",
    "  value_counts = pd.value_counts(y)\n",
    "  categories = value_counts[\n",
    "      value_counts >= frequency_threshold].index.to_numpy()\n",
    "  # 0 indicates the unknown category.\n",
    "  return pd.Categorical(y, categories=categories).codes + 1\n",
    "\n",
    "for key in VOCAB_FEATURES:\n",
    "  df[key] = label_encoding(df[key])\n",
    "\n",
    "MAIL_ORDER_RESPONSES = [\n",
    "    'MBCRAFT',\n",
    "    'MBGARDEN',\n",
    "    'MBBOOKS',\n",
    "    'MBCOLECT',\n",
    "    'MAGFAML',\n",
    "    'MAGFEM',\n",
    "    'MAGMALE',\n",
    "    'PUBGARDN',\n",
    "    'PUBCULIN',\n",
    "    'PUBHLTH',\n",
    "    'PUBDOITY',\n",
    "    'PUBNEWFN',\n",
    "    'PUBPHOTO',\n",
    "    'PUBOPP',\n",
    "    'RFA_2F',\n",
    "]\n",
    "\n",
    "INDICATOR_FEATURES = [\n",
    "    'AGE',  # age decile, 0 indicates unknown\n",
    "    'NUMCHLD',\n",
    "    'INCOME',\n",
    "    'WEALTH1',\n",
    "    'HIT',\n",
    "] + MAIL_ORDER_RESPONSES\n",
    "\n",
    "df['AGE'] = pd.qcut(df['AGE'].values, 10).codes + 1\n",
    "df['NUMCHLD'] = df['NUMCHLD'].apply(lambda x: 0 if np.isnan(x) else int(x))\n",
    "df['INCOME'] = df['INCOME'].apply(lambda x: 0 if np.isnan(x) else int(x))\n",
    "df['WEALTH1'] = df['WEALTH1'].apply(lambda x: 0 if np.isnan(x) else int(x) + 1)\n",
    "df['HIT'] = pd.qcut(df['HIT'].values, q=50, duplicates='drop').codes\n",
    "\n",
    "for col in MAIL_ORDER_RESPONSES:\n",
    "  df[col] = pd.qcut(df[col].values, q=20, duplicates='drop').codes + 1\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    # binary\n",
    "    'MAILCODE',  # bad address\n",
    "    'NOEXCH',    # do not exchange\n",
    "    'RECINHSE',  # donor has given to PVA's in house program\n",
    "    'RECP3',     # donor has given to PVA's P3 program\n",
    "    'RECPGVG',   # planned giving record\n",
    "    'RECSWEEP',  # sweepstakes record\n",
    "    'HOMEOWNR',  # home owner\n",
    "    'CHILD03',\n",
    "    'CHILD07',\n",
    "    'CHILD12',\n",
    "    'CHILD18',\n",
    "\n",
    "    # continuous\n",
    "    'CARDPROM',\n",
    "    'NUMPROM',\n",
    "    'CARDPM12',\n",
    "    'NUMPRM12',\n",
    "    'RAMNTALL',\n",
    "    'NGIFTALL',\n",
    "    'MINRAMNT',\n",
    "    'MAXRAMNT',\n",
    "    'LASTGIFT',\n",
    "    'AVGGIFT',\n",
    "]\n",
    "     \n",
    "\n",
    "df['MAILCODE'] = (df['MAILCODE'] == 'B').astype('float32')\n",
    "df['PVASTATE'] = df['PVASTATE'].isin(['P', 'E']).astype('float32')\n",
    "df['NOEXCH'] = df['NOEXCH'].isin(['X', '1']).astype('float32')\n",
    "df['RECINHSE'] = (df['RECINHSE'] == 'X').astype('float32')\n",
    "df['RECP3'] = (df['RECP3'] == 'X').astype('float32')\n",
    "df['RECPGVG'] = (df['RECPGVG'] == 'X').astype('float32')\n",
    "df['RECSWEEP'] = (df['RECSWEEP'] == 'X').astype('float32')\n",
    "df['HOMEOWNR'] = (df['HOMEOWNR'] == 'H').astype('float32')\n",
    "df['CHILD03'] = df['CHILD03'].isin(['M', 'F', 'B']).astype('float32')\n",
    "df['CHILD07'] = df['CHILD07'].isin(['M', 'F', 'B']).astype('float32')\n",
    "df['CHILD12'] = df['CHILD12'].isin(['M', 'F', 'B']).astype('float32')\n",
    "df['CHILD18'] = df['CHILD18'].isin(['M', 'F', 'B']).astype('float32')\n",
    "\n",
    "df['CARDPROM'] = df['CARDPROM'] / 100\n",
    "df['NUMPROM'] = df['NUMPROM'] / 100\n",
    "df['CARDPM12'] = df['CARDPM12'] / 100\n",
    "df['NUMPRM12'] = df['NUMPRM12'] / 100\n",
    "df['RAMNTALL'] = np.log1p(df['RAMNTALL'])\n",
    "df['NGIFTALL'] = np.log1p(df['NGIFTALL'])\n",
    "df['MINRAMNT'] = np.log1p(df['MINRAMNT'])\n",
    "df['MAXRAMNT'] = np.log1p(df['MAXRAMNT'])\n",
    "df['LASTGIFT'] = np.log1p(df['LASTGIFT'])\n",
    "df['AVGGIFT'] = np.log1p(df['AVGGIFT'])\n",
    "\n",
    "CATEGORICAL_FEATURES = VOCAB_FEATURES + INDICATOR_FEATURES\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERIC_FEATURES\n",
    "\n",
    "def dnn_split(df):\n",
    "  df_train = df.iloc[:num_train]\n",
    "  df_eval = df.iloc[num_train:]\n",
    "\n",
    "  def feature_dict(df):\n",
    "    features = {k: v.values.reshape(-1,1) for k, v in dict(df[CATEGORICAL_FEATURES]).items()}\n",
    "    features['numeric'] = df[NUMERIC_FEATURES].astype('float32').values\n",
    "    return features\n",
    "\n",
    "  x_train, y_train = feature_dict(df_train), df_train['TARGET_D'].astype(\n",
    "      'float32').values\n",
    "  x_eval, y_eval = feature_dict(df_eval), df_eval['TARGET_D'].astype(\n",
    "      'float32').values\n",
    "\n",
    "  return x_train, x_eval, y_train, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a400356b-9f25-4ad3-a97b-0f6aecdc468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_eval, y_train, y_eval = dnn_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ceb409-ede6-41bc-a919-2aa5dff6b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [{0:[np.hstack(list(x_train.values())),y_train,np.hstack(list(x_eval.values())),y_eval]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1fdf5-56e6-4d06-aa96-cf692d333400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316d860-a57a-49c6-8171-38c550ab3dbb",
   "metadata": {},
   "source": [
    "# Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54501dee-6254-44db-9937-868b4c06025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574c2b2-ce44-4403-9b55-889c67f73e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.append({0:[X,y]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64bd5c-79eb-4b8c-91f3-fb1cbdb9afde",
   "metadata": {},
   "source": [
    "# California housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02940a94-1aa6-4cb4-87cd-7b86032f61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "all_data.append({0:[X.to_numpy(),y.to_numpy()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38685a8-b33e-45e6-8beb-c91582a811bf",
   "metadata": {},
   "source": [
    "# Liver disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49e30a-318f-4100-a288-57c2a4875361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "ld = fetch_openml(name='liver-disorders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c19aa-3f9a-476c-9d49-29ad4e8b70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ld['data'].to_numpy(),ld['target'].to_numpy()\n",
    "all_data.append({0:[X,y]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393b553-2de8-4e45-9c6f-91985f24c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "dict_data = {}\n",
    "dict_data[\"Diabetes\"] = {}\n",
    "dict_data[\"California housing\"] = {}\n",
    "dict_data[\"Liver disorders\"] = {}\n",
    "dict_data[\"KDD98\"] = {}\n",
    "\n",
    "for k in all_data[0]:\n",
    "    x01,x02,y01,y02 = all_data[0][k][0], all_data[0][k][2], all_data[0][k][1], all_data[0][k][3]\n",
    "    dict_data[\"KDD98\"][k] = {\"train\":{\"X\":x01,\"y\":y01},\"test\":{\"X\":x02,\"y\":y02}}\n",
    "\n",
    "for k in all_data[1]:\n",
    "    x01,x02,y01,y02 = train_test_split(all_data[1][k][0], all_data[1][k][1], test_size=0.3,random_state=42)\n",
    "    dict_data[\"Diabetes\"][k] = {\"train\":{\"X\":x01,\"y\":y01},\"test\":{\"X\":x02,\"y\":y02}}\n",
    "    \n",
    "for k in all_data[2]:\n",
    "    x11,x12,y11,y12 = train_test_split(all_data[2][k][0], all_data[2][k][1],test_size=0.3,random_state=42)\n",
    "    dict_data[\"California housing\"][k] = {\"train\":{\"X\":x11,\"y\":y11},\"test\":{\"X\":x12,\"y\":y12}} \n",
    "\n",
    "for k in all_data[3]:\n",
    "    x11,x12,y11,y12 = train_test_split(all_data[3][k][0], all_data[3][k][1],test_size=0.3,random_state=42)\n",
    "    dict_data[\"Liver disorders\"][k] = {\"train\":{\"X\":x11,\"y\":y11},\"test\":{\"X\":x12,\"y\":y12}} \n",
    "\n",
    "all_data = dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c9f65-627f-4384-9c0a-3d453d53dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"KDD98\"][0][\"train\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b17d72-8e18-4e6c-8c98-a87e573bcda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"KDD98\"][0][\"train\"][\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb83ffb-2182-459c-a41a-8cee2fd5db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"KDD98\"][0][\"test\"][\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a61e1-cfd8-44e9-9690-d4588829beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"KDD98\"][0][\"test\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06f10d-26fe-4c58-898b-dd6ce7965e95",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbd0f0-9e06-477e-9e59-2e86be3d3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "np.bool = np.bool_\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelXGB(max_depth,layers,C):\n",
    "    return xgb.XGBRegressor(max_depth = max_depth, n_estimators = layers)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C):\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4,backend=\"sklearn\",criterion='squared_error')\n",
    "\n",
    "def make_modelBoosted(max_depth,layers,C,hs):\n",
    "    return CascadeBoostingRegressor(C=C, n_layers=layers, n_estimators = 4, max_depth=max_depth, n_iter_no_change = 1, validation_fraction = 0.1, learning_rate = 0.9,hidden_size = hs)\n",
    "\n",
    "\n",
    "models = {\"XGB\":make_modelXGB,\"Cascade Forest\":make_modelCascade, \"Boosted Forest\": make_modelBoosted}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 1, 10)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 10, 2000)\n",
    "                    hs = trial.suggest_int('hs', 2, 20)\n",
    "                else:\n",
    "                    C = 0\n",
    "                    hs = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    if hs > 0:\n",
    "                        model = make_model(max_depth,layers,C,hs)\n",
    "                    else:    \n",
    "                        model = make_model(max_depth,layers,C)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=100)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]\n",
    "                hs = study.best_trial.params[\"C\"]\n",
    "            else:\n",
    "                C = 0\n",
    "                hs = 0\n",
    "            if hs > 0:    \n",
    "                model = make_model(max_depth,layers,C,hs)\n",
    "            else:\n",
    "                model = make_model(max_depth,layers,C)\n",
    "                \n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"classic_datasets/boosting_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5840ea-6dbd-4908-bc15-8433ea5f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a6484-c8b1-45ad-a46b-3a44fd639fd4",
   "metadata": {},
   "source": [
    "# Alternative weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf1433-5a85-4345-8290-bc679d44b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ecdfr.gcForest import gcForest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelECDFR(max_depth,layers,resampling_rate, et):\n",
    "    config = {\"estimator_configs\":[{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth}],\n",
    "              \"error_threshold\": et,\n",
    "              \"resampling_rate\": resampling_rate,\n",
    "              \"random_state\":None,\n",
    "              \"max_layers\":layers,\n",
    "              \"early_stop_rounds\":1,\n",
    "              \"train_evaluation\":mean_squared_error}\n",
    "    \n",
    "    return gcForest(config,1)\n",
    "\n",
    "models = {\"ecdfr\":make_modelECDFR}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                C = trial.suggest_float('resampling_rate', 1.1, 1.8)\n",
    "                et = trial.suggest_float('et', 0.01, 0.9)\n",
    "                \n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                try:\n",
    "                    for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                        model = make_modelECDFR(max_depth,layers,C,et)\n",
    "                    \n",
    "                        model.fit(\n",
    "                             x_train[train_index],\n",
    "                             Y_train[train_index],\n",
    "                        )\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                except:\n",
    "                    scores = [1000000000.]\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=1000)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "\n",
    "            C = study.best_trial.params[\"resampling_rate\"]  \n",
    "            et = study.best_trial.params[\"et\"]  \n",
    "            model = make_model(max_depth,layers,C,et)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            print(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max())     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589b187-466d-4640-b212-b80afb874abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C,wt):\n",
    "    wf = {0:\"linear\", 1:\"1-w^1/2\", 2:\"1-w2\"}\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4,adaptive=True,weighting_function = wf[wt])\n",
    "\n",
    "\n",
    "models = {\"AWDF\":make_modelCascade}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "                wt = trial.suggest_int('weight_function', 0, 2)   \n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 1, 2000)\n",
    "                else:\n",
    "                    C = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(max_depth,layers,C,wt)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=50)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "            wt = study.best_trial.params['weight_function']\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]  \n",
    "            else:\n",
    "                C = 0\n",
    "            model = make_model(max_depth,layers,C,wt)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"classic_datasets/awdf_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d598c-6512-4aa4-964b-b562d1998be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
