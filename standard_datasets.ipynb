{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba409a4f-6e35-4614-818f-f2272c3c157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7f3b2-7f98-424e-bc42-9480cc40f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830acedf-3cf2-4fbd-ac93-3a43c95ae8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ebf8c2-5dbe-46d4-b1a1-29f0588b18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "def printf(*args, fname=\"log.txt\"):\n",
    "    with open(join(\"test_outputs\",fname),\"a+\") as f:\n",
    "        for a in args:\n",
    "            f.write(str(a) + \" \")\n",
    "        f.write(\"\\n\") \n",
    "    print(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315515b-ae62-4adf-a08f-f08f76e46be0",
   "metadata": {},
   "source": [
    "# KDD 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5424e065-2578-404c-b9e5-0ef4f71beaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p lifetime-value/kdd_cup_98\n",
    "#!wget https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98lrn.zip -P lifetime-value/kdd_cup_98/\n",
    "#!wget https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98val.zip -P lifetime-value/kdd_cup_98/\n",
    "#!wget https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/valtargt.txt -P lifetime-value/kdd_cup_98/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbcdebe-71d6-469b-a1bc-9e80ea80df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip lifetime-value/kdd_cup_98/cup98lrn.zip\n",
    "#!unzip lifetime-value/kdd_cup_98/cup98val.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdef8981-341c-4e79-9751-14deac11a27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62563/3556400376.py:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv('cup98LRN.txt')\n",
      "/tmp/ipykernel_62563/3556400376.py:6: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_eval = pd.read_csv('cup98VAL.txt')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv('cup98LRN.txt')\n",
    "num_train = df_train.shape[0]\n",
    "df_eval = pd.read_csv('cup98VAL.txt')\n",
    "df_eval_target = pd.read_csv('lifetime-value/kdd_cup_98/valtargt.txt')\n",
    "df_eval = df_eval.merge(df_eval_target, on='CONTROLN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6410da4-5ce8-4e55-9b24-333082e182fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n",
      "/tmp/ipykernel_62563/2921462118.py:32: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  value_counts = pd.value_counts(y)\n"
     ]
    }
   ],
   "source": [
    "# Original: https://github.com/google/lifetime_value/blob/master/notebooks/kdd_cup_98/regression.ipynb\n",
    "df = pd.concat([df_train, df_eval], axis=0, sort=True)\n",
    "y = df['TARGET_D'][:num_train]\n",
    "\n",
    "VOCAB_FEATURES = [\n",
    "    'ODATEDW',  # date of donor's first gift (YYMM)\n",
    "    'OSOURCE',  # donor acquisition mailing list\n",
    "    'TCODE',    # donor title code\n",
    "    'STATE',\n",
    "    'ZIP',\n",
    "    'DOMAIN',   # urbanicity level and socio-economic status of the neighborhood\n",
    "    'CLUSTER',  # socio-economic status\n",
    "    'GENDER',\n",
    "    'MAXADATE', # date of the most recent promotion received\n",
    "    'MINRDATE',\n",
    "    'LASTDATE',\n",
    "    'FISTDATE',\n",
    "    'RFA_2A',\n",
    "]\n",
    "\n",
    "df['ODATEDW'] = df['ODATEDW'].astype('str')\n",
    "df['TCODE'] = df['TCODE'].apply(\n",
    "    lambda x: '{:03d}'.format(x // 1000 if x > 1000 else x))\n",
    "df['ZIP'] = df['ZIP'].str.slice(0, 5)\n",
    "df['MAXADATE'] = df['MAXADATE'].astype('str')\n",
    "df['MINRDATE'] = df['MINRDATE'].astype('str')\n",
    "df['LASTDATE'] = df['LASTDATE'].astype('str')\n",
    "df['FISTDATE'] = df['FISTDATE'].astype('str')\n",
    "\n",
    "\n",
    "def label_encoding(y, frequency_threshold=100):\n",
    "  value_counts = pd.value_counts(y)\n",
    "  categories = value_counts[\n",
    "      value_counts >= frequency_threshold].index.to_numpy()\n",
    "  # 0 indicates the unknown category.\n",
    "  return pd.Categorical(y, categories=categories).codes + 1\n",
    "\n",
    "for key in VOCAB_FEATURES:\n",
    "  df[key] = label_encoding(df[key])\n",
    "\n",
    "MAIL_ORDER_RESPONSES = [\n",
    "    'MBCRAFT',\n",
    "    'MBGARDEN',\n",
    "    'MBBOOKS',\n",
    "    'MBCOLECT',\n",
    "    'MAGFAML',\n",
    "    'MAGFEM',\n",
    "    'MAGMALE',\n",
    "    'PUBGARDN',\n",
    "    'PUBCULIN',\n",
    "    'PUBHLTH',\n",
    "    'PUBDOITY',\n",
    "    'PUBNEWFN',\n",
    "    'PUBPHOTO',\n",
    "    'PUBOPP',\n",
    "    'RFA_2F',\n",
    "]\n",
    "\n",
    "INDICATOR_FEATURES = [\n",
    "    'AGE',  # age decile, 0 indicates unknown\n",
    "    'NUMCHLD',\n",
    "    'INCOME',\n",
    "    'WEALTH1',\n",
    "    'HIT',\n",
    "] + MAIL_ORDER_RESPONSES\n",
    "\n",
    "df['AGE'] = pd.qcut(df['AGE'].values, 10).codes + 1\n",
    "df['NUMCHLD'] = df['NUMCHLD'].apply(lambda x: 0 if np.isnan(x) else int(x))\n",
    "df['INCOME'] = df['INCOME'].apply(lambda x: 0 if np.isnan(x) else int(x))\n",
    "df['WEALTH1'] = df['WEALTH1'].apply(lambda x: 0 if np.isnan(x) else int(x) + 1)\n",
    "df['HIT'] = pd.qcut(df['HIT'].values, q=50, duplicates='drop').codes\n",
    "\n",
    "for col in MAIL_ORDER_RESPONSES:\n",
    "  df[col] = pd.qcut(df[col].values, q=20, duplicates='drop').codes + 1\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    # binary\n",
    "    'MAILCODE',  # bad address\n",
    "    'NOEXCH',    # do not exchange\n",
    "    'RECINHSE',  # donor has given to PVA's in house program\n",
    "    'RECP3',     # donor has given to PVA's P3 program\n",
    "    'RECPGVG',   # planned giving record\n",
    "    'RECSWEEP',  # sweepstakes record\n",
    "    'HOMEOWNR',  # home owner\n",
    "    'CHILD03',\n",
    "    'CHILD07',\n",
    "    'CHILD12',\n",
    "    'CHILD18',\n",
    "\n",
    "    # continuous\n",
    "    'CARDPROM',\n",
    "    'NUMPROM',\n",
    "    'CARDPM12',\n",
    "    'NUMPRM12',\n",
    "    'RAMNTALL',\n",
    "    'NGIFTALL',\n",
    "    'MINRAMNT',\n",
    "    'MAXRAMNT',\n",
    "    'LASTGIFT',\n",
    "    'AVGGIFT',\n",
    "]\n",
    "     \n",
    "\n",
    "df['MAILCODE'] = (df['MAILCODE'] == 'B').astype('float32')\n",
    "df['PVASTATE'] = df['PVASTATE'].isin(['P', 'E']).astype('float32')\n",
    "df['NOEXCH'] = df['NOEXCH'].isin(['X', '1']).astype('float32')\n",
    "df['RECINHSE'] = (df['RECINHSE'] == 'X').astype('float32')\n",
    "df['RECP3'] = (df['RECP3'] == 'X').astype('float32')\n",
    "df['RECPGVG'] = (df['RECPGVG'] == 'X').astype('float32')\n",
    "df['RECSWEEP'] = (df['RECSWEEP'] == 'X').astype('float32')\n",
    "df['HOMEOWNR'] = (df['HOMEOWNR'] == 'H').astype('float32')\n",
    "df['CHILD03'] = df['CHILD03'].isin(['M', 'F', 'B']).astype('float32')\n",
    "df['CHILD07'] = df['CHILD07'].isin(['M', 'F', 'B']).astype('float32')\n",
    "df['CHILD12'] = df['CHILD12'].isin(['M', 'F', 'B']).astype('float32')\n",
    "df['CHILD18'] = df['CHILD18'].isin(['M', 'F', 'B']).astype('float32')\n",
    "\n",
    "df['CARDPROM'] = df['CARDPROM'] / 100\n",
    "df['NUMPROM'] = df['NUMPROM'] / 100\n",
    "df['CARDPM12'] = df['CARDPM12'] / 100\n",
    "df['NUMPRM12'] = df['NUMPRM12'] / 100\n",
    "df['RAMNTALL'] = np.log1p(df['RAMNTALL'])\n",
    "df['NGIFTALL'] = np.log1p(df['NGIFTALL'])\n",
    "df['MINRAMNT'] = np.log1p(df['MINRAMNT'])\n",
    "df['MAXRAMNT'] = np.log1p(df['MAXRAMNT'])\n",
    "df['LASTGIFT'] = np.log1p(df['LASTGIFT'])\n",
    "df['AVGGIFT'] = np.log1p(df['AVGGIFT'])\n",
    "\n",
    "CATEGORICAL_FEATURES = VOCAB_FEATURES + INDICATOR_FEATURES\n",
    "ALL_FEATURES = CATEGORICAL_FEATURES + NUMERIC_FEATURES\n",
    "\n",
    "def dnn_split(df):\n",
    "  df_train = df.iloc[:num_train]\n",
    "  df_eval = df.iloc[num_train:]\n",
    "\n",
    "  def feature_dict(df):\n",
    "    features = {k: v.values.reshape(-1,1) for k, v in dict(df[CATEGORICAL_FEATURES]).items()}\n",
    "    features['numeric'] = df[NUMERIC_FEATURES].astype('float32').values\n",
    "    return features\n",
    "\n",
    "  x_train, y_train = feature_dict(df_train), df_train['TARGET_D'].astype(\n",
    "      'float32').values\n",
    "  x_eval, y_eval = feature_dict(df_eval), df_eval['TARGET_D'].astype(\n",
    "      'float32').values\n",
    "\n",
    "  return x_train, x_eval, y_train, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a400356b-9f25-4ad3-a97b-0f6aecdc468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_eval, y_train, y_eval = dnn_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ceb409-ede6-41bc-a919-2aa5dff6b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [{0:[np.hstack(list(x_train.values())),y_train,np.hstack(list(x_eval.values())),y_eval]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a1fdf5-56e6-4d06-aa96-cf692d333400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316d860-a57a-49c6-8171-38c550ab3dbb",
   "metadata": {},
   "source": [
    "# Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54501dee-6254-44db-9937-868b4c06025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5574c2b2-ce44-4403-9b55-889c67f73e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.append({0:[X,y]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64bd5c-79eb-4b8c-91f3-fb1cbdb9afde",
   "metadata": {},
   "source": [
    "# California housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02940a94-1aa6-4cb4-87cd-7b86032f61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "all_data.append({0:[X.to_numpy(),y.to_numpy()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38685a8-b33e-45e6-8beb-c91582a811bf",
   "metadata": {},
   "source": [
    "# Liver disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a49e30a-318f-4100-a288-57c2a4875361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "ld = fetch_openml(name='liver-disorders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "251c19aa-3f9a-476c-9d49-29ad4e8b70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ld['data'].to_numpy(),ld['target'].to_numpy()\n",
    "all_data.append({0:[X,y]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2393b553-2de8-4e45-9c6f-91985f24c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "dict_data = {}\n",
    "dict_data[\"Diabetes\"] = {}\n",
    "dict_data[\"California housing\"] = {}\n",
    "dict_data[\"Liver disorders\"] = {}\n",
    "dict_data[\"KDD98\"] = {}\n",
    "\n",
    "for k in all_data[0]:\n",
    "    x01,x02,y01,y02 = all_data[0][k][0], all_data[0][k][2], all_data[0][k][1], all_data[0][k][3]\n",
    "    dict_data[\"KDD98\"][k] = {\"train\":{\"X\":x01,\"y\":y01},\"test\":{\"X\":x02,\"y\":y02}}\n",
    "\n",
    "for k in all_data[1]:\n",
    "    x01,x02,y01,y02 = train_test_split(all_data[1][k][0], all_data[1][k][1], test_size=0.3,random_state=42)\n",
    "    dict_data[\"Diabetes\"][k] = {\"train\":{\"X\":x01,\"y\":y01},\"test\":{\"X\":x02,\"y\":y02}}\n",
    "    \n",
    "for k in all_data[2]:\n",
    "    x11,x12,y11,y12 = train_test_split(all_data[2][k][0], all_data[2][k][1],test_size=0.3,random_state=42)\n",
    "    dict_data[\"California housing\"][k] = {\"train\":{\"X\":x11,\"y\":y11},\"test\":{\"X\":x12,\"y\":y12}} \n",
    "\n",
    "for k in all_data[3]:\n",
    "    x11,x12,y11,y12 = train_test_split(all_data[3][k][0], all_data[3][k][1],test_size=0.3,random_state=42)\n",
    "    dict_data[\"Liver disorders\"][k] = {\"train\":{\"X\":x11,\"y\":y11},\"test\":{\"X\":x12,\"y\":y12}} \n",
    "\n",
    "all_data = dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f1c9f65-627f-4384-9c0a-3d453d53dffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95412, 54)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"KDD98\"][0][\"train\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b17d72-8e18-4e6c-8c98-a87e573bcda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95412,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"KDD98\"][0][\"train\"][\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb83ffb-2182-459c-a41a-8cee2fd5db15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96367,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"KDD98\"][0][\"test\"][\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585a61e1-cfd8-44e9-9690-d4588829beda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96367, 54)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"KDD98\"][0][\"test\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06f10d-26fe-4c58-898b-dd6ce7965e95",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75cbd0f0-9e06-477e-9e59-2e86be3d3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-05-16 22:45:50,869] A new study created in memory with name: no-name-2587f21b-d027-4411-a002-489e655160ad\n",
      "/usr/local/lib/python3.9/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "[W 2025-05-16 22:46:54,336] Trial 0 failed with parameters: {'layers': 7, 'max_depth': 2, 'C': 1121, 'hs': 2, 'trees': 56} because of the following error: UnboundLocalError(\"local variable 'train_loader' referenced before assignment\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_62563/2736455390.py\", line 63, in objective\n",
      "    model.fit(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/workspace/notebooks/soil_temp/boosted_forest.py\", line 250, in fit\n",
      "    n_stages = self._fit_stages(\n",
      "  File \"/workspace/notebooks/soil_temp/boosted_forest.py\", line 465, in _fit_stages\n",
      "    raw_predictions, history_sum = self._fit_stage(\n",
      "  File \"/workspace/notebooks/soil_temp/boosted_forest.py\", line 660, in _fit_stage\n",
      "    raw_predictions, history_sum = self.update_terminal_regions(self.estimators_[i],trains, tests,X_aug, y,history_sum,sample_weight)\n",
      "  File \"/workspace/notebooks/soil_temp/boosted_forest.py\", line 704, in update_terminal_regions\n",
      "    cur_lr.fit(I, y, trains, tests, bias = history_sum, sample_weight = sample_weight)\n",
      "  File \"/workspace/notebooks/soil_temp/torch_mlp.py\", line 169, in fit\n",
      "    for tensors in train_loader:\n",
      "UnboundLocalError: local variable 'train_loader' referenced before assignment\n",
      "[W 2025-05-16 22:46:54,337] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'train_loader' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(scores)\u001b[38;5;241m.\u001b[39mmean() \n\u001b[1;32m     71\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     74\u001b[0m layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;66;03m#study.best_trial.params[\"layers\"]  \u001b[39;00m\n\u001b[1;32m     75\u001b[0m max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;66;03m#study.best_trial.params[\"max_depth\"]  \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[19], line 63\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[1;32m     61\u001b[0m     model \u001b[38;5;241m=\u001b[39m make_model(max_depth,layers,C,n_trees)\n\u001b[0;32m---> 63\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m     \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m     \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_train[test_index]) \u001b[38;5;66;03m#, batch_size=batch_size)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(mean_squared_error(Y_train[test_index]\u001b[38;5;241m.\u001b[39mflatten(),y_pred\u001b[38;5;241m.\u001b[39mflatten()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/notebooks/soil_temp/boosted_forest.py:250\u001b[0m, in \u001b[0;36mBaseBoostedCascade.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_):\n",
      "File \u001b[0;32m/workspace/notebooks/soil_temp/boosted_forest.py:465\u001b[0m, in \u001b[0;36mBaseBoostedCascade._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    458\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    459\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    460\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    461\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    462\u001b[0m         )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m raw_predictions, history_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m/workspace/notebooks/soil_temp/boosted_forest.py:660\u001b[0m, in \u001b[0;36mBaseBoostedCascade._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, history_sum, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    658\u001b[0m     tests \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tests_\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[i]\u001b[38;5;241m.\u001b[39mappend(kfold_estimator)\n\u001b[0;32m--> 660\u001b[0m raw_predictions, history_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_terminal_regions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtests\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhistory_sum\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# add tree to ensemble\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_predictions, history_sum\n",
      "File \u001b[0;32m/workspace/notebooks/soil_temp/boosted_forest.py:704\u001b[0m, in \u001b[0;36mBaseBoostedCascade.update_terminal_regions\u001b[0;34m(self, estimators, trains, tests, X, y, history_sum, sample_weight)\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ek\u001b[38;5;241m.\u001b[39mestimators_:\n\u001b[1;32m    702\u001b[0m         I\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetIndicators(e, X, do_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 704\u001b[0m \u001b[43mcur_lr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhistory_sum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m raw_predictions, hidden \u001b[38;5;241m=\u001b[39m cur_lr\u001b[38;5;241m.\u001b[39mdecision_function(I,tests,history_sum)\n\u001b[1;32m    706\u001b[0m rp, _ \u001b[38;5;241m=\u001b[39m cur_lr\u001b[38;5;241m.\u001b[39mdecision_function(I,\u001b[38;5;28;01mNone\u001b[39;00m,history_sum)  \n",
      "File \u001b[0;32m/workspace/notebooks/soil_temp/torch_mlp.py:169\u001b[0m, in \u001b[0;36mMLPRB.fit\u001b[0;34m(self, X, y, indexes, test_indexes, bias, sample_weight)\u001b[0m\n\u001b[1;32m    167\u001b[0m eloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    168\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tensors \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m    170\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'train_loader' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "np.bool = np.bool_\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelXGB(max_depth,layers,C,n_trees):\n",
    "    return xgb.XGBRegressor(max_depth = max_depth, n_estimators = layers)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C,n_trees):\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4,backend=\"sklearn\",criterion='squared_error',n_trees=n_trees)\n",
    "\n",
    "def make_modelBoosted(max_depth,layers,C,hs,n_trees):\n",
    "    return CascadeBoostingRegressor(C=C, n_layers=layers, n_estimators = 4, max_depth=max_depth, n_iter_no_change = 1, validation_fraction = 0.1, learning_rate = 0.9,hidden_size = hs,n_trees=n_trees)\n",
    "\n",
    "\n",
    "#models = {\"XGB\":make_modelXGB,\"Cascade Forest\":make_modelCascade, \"Boosted Forest\": make_modelBoosted}\n",
    "models = {\"Boosted Forest\": make_modelBoosted}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in [\"KDD98\"]:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 1, 10)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 10, 2000)\n",
    "                    hs = trial.suggest_int('hs', 2, 8)\n",
    "                    n_trees = trial.suggest_int('trees', 10, 100)\n",
    "                else:\n",
    "                    if model_name == \"Cascade Forest\":\n",
    "                        n_trees = trial.suggest_int('trees', 10, 100)\n",
    "                    else:\n",
    "                        n_trees = 0    \n",
    "                    C = 0\n",
    "                    hs = 0\n",
    "\n",
    "                scores = []\n",
    "                for _ in range(3):\n",
    "                    kf = KFold(n_splits=5)\n",
    "                    for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                        if hs > 0:\n",
    "                            model = make_model(max_depth,layers,C,hs,n_trees)\n",
    "                        else:    \n",
    "                            model = make_model(max_depth,layers,C,n_trees)\n",
    "                        \n",
    "                        model.fit(\n",
    "                             x_train[train_index],\n",
    "                             Y_train[train_index],\n",
    "                        )\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=100)    \n",
    "            \n",
    "            layers = 2#study.best_trial.params[\"layers\"]  \n",
    "            max_depth = 1#study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]\n",
    "                hs = study.best_trial.params[\"hs\"]\n",
    "                n_trees = study.best_trial.params[\"trees\"]\n",
    "            else:\n",
    "                if model_name == \"Cascade Forest\":\n",
    "                    n_trees = study.best_trial.params[\"trees\"]\n",
    "                else:\n",
    "                    n_trees = 0\n",
    "                C = 0\n",
    "                hs = 0\n",
    "            if hs > 0:    \n",
    "                model = make_model(max_depth,layers,C,hs,n_trees)\n",
    "            else:\n",
    "                model = make_model(max_depth,layers,C,n_trees)\n",
    "                \n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"classic_datasets/boosting_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea91575-2ecf-4bc5-b2e5-3644d386d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a82fd-956a-413e-a586-95e6aa0cc9e5",
   "metadata": {},
   "source": [
    "# Build a grid-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9878eba-e9aa-4d7b-bacf-d217f500d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "np.bool = np.bool_\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelXGB(max_depth,layers,C):\n",
    "    return xgb.XGBRegressor(max_depth = max_depth, n_estimators = layers)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C):\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4,backend=\"sklearn\",criterion='squared_error')\n",
    "\n",
    "def make_modelBoosted(max_depth,layers,C,hs):\n",
    "    return CascadeBoostingRegressor(C=C, n_layers=layers, n_estimators = 4, max_depth=max_depth, n_iter_no_change = 1, validation_fraction = 0.1, learning_rate = 0.9,hidden_size = hs)\n",
    "\n",
    "\n",
    "#models = {\"XGB\":make_modelXGB,\"Cascade Forest\":make_modelCascade, \"Boosted Forest\": make_modelBoosted}\n",
    "models = {\"Boosted Forest\": make_modelBoosted}\n",
    "\n",
    "bo_data = {}    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in [\"Diabetes\"]:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            for layers in [1,2,3,4,5]:\n",
    "                for hs in [1,2,3,4,5,6,7,8]:\n",
    "                    scores = []\n",
    "                    scores2 = []\n",
    "                    for _ in range(3):\n",
    "                        max_depth = 1 #trial.suggest_int('max_depth', 1, 2)\n",
    "                        C = 100 #trial.suggest_int('C', 10, 2000)\n",
    "                        kf = KFold(n_splits=5)\n",
    "                        \n",
    "                        for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                            if hs > 0:\n",
    "                                model = make_model(max_depth,layers,C,hs)\n",
    "                            else:    \n",
    "                                model = make_model(max_depth,layers,C)\n",
    "                            \n",
    "                            model.fit(\n",
    "                                 x_train[train_index],\n",
    "                                 Y_train[train_index],\n",
    "                            )\n",
    "                            y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                            y_pred2 = model.predict(x_test) #, batch_size=batch_size)\n",
    "                            scores.append([mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten())])\n",
    "                            scores2.append([mean_squared_error(Y_test.flatten(),y_pred2.flatten())])\n",
    "                    bo_data[(layers,hs)] = [np.asarray(scores).mean(),np.asarray(scores2).mean()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d8057-7ed1-4827-a0fd-05ec0a727e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5840ea-6dbd-4908-bc15-8433ea5f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('bo_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(bo_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c30ae-8ad7-46ac-afb2-ac848f16d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a6484-c8b1-45ad-a46b-3a44fd639fd4",
   "metadata": {},
   "source": [
    "# Alternative weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf1433-5a85-4345-8290-bc679d44b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from ecdfr.gcForest import gcForest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelECDFR(max_depth,layers,resampling_rate, et):\n",
    "    config = {\"estimator_configs\":[{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth}],\n",
    "              \"error_threshold\": et,\n",
    "              \"resampling_rate\": resampling_rate,\n",
    "              \"random_state\":None,\n",
    "              \"max_layers\":layers,\n",
    "              \"early_stop_rounds\":1,\n",
    "              \"train_evaluation\":mean_squared_error}\n",
    "    \n",
    "    return gcForest(config,1)\n",
    "\n",
    "models = {\"ecdfr\":make_modelECDFR}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                C = trial.suggest_float('resampling_rate', 1.1, 1.8)\n",
    "                et = trial.suggest_float('et', 0.01, 0.9)\n",
    "                \n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                try:\n",
    "                    for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                        model = make_modelECDFR(max_depth,layers,C,et)\n",
    "                    \n",
    "                        model.fit(\n",
    "                             x_train[train_index],\n",
    "                             Y_train[train_index],\n",
    "                        )\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                except:\n",
    "                    scores = [1000000000.]\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=1000)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "\n",
    "            C = study.best_trial.params[\"resampling_rate\"]  \n",
    "            et = study.best_trial.params[\"et\"]  \n",
    "            model = make_model(max_depth,layers,C,et)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            print(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max())     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589b187-466d-4640-b212-b80afb874abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C,wt):\n",
    "    wf = {0:\"linear\", 1:\"1-w^1/2\", 2:\"1-w2\"}\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4,adaptive=True,weighting_function = wf[wt])\n",
    "\n",
    "\n",
    "models = {\"AWDF\":make_modelCascade}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "                wt = trial.suggest_int('weight_function', 0, 2)   \n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 1, 2000)\n",
    "                else:\n",
    "                    C = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(max_depth,layers,C,wt)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=50)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "            wt = study.best_trial.params['weight_function']\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]  \n",
    "            else:\n",
    "                C = 0\n",
    "            model = make_model(max_depth,layers,C,wt)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"classic_datasets/awdf_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d598c-6512-4aa4-964b-b562d1998be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
