{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1e02a-e59e-4095-ae53-f13e6ee96ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c8e03-889b-4a36-96ba-ce156aa2cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324622c3-29c6-4eae-b94d-b521f2a55961",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8bb49-5d14-425d-a85a-11daddd678d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc0415-9dbc-486f-8ae0-f1598e0132b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10813341-60ea-45d8-be93-2fbf0a81212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "def printf(*args, fname=\"log.txt\"):\n",
    "    with open(join(\"test_outputs\",fname),\"a+\") as f:\n",
    "        for a in args:\n",
    "            f.write(str(a) + \" \")\n",
    "        f.write(\"\\n\") \n",
    "    print(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280cc48-46c8-478b-b6f6-cf7d35431001",
   "metadata": {},
   "source": [
    "# Load Reynolds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7d689-cbb3-45b1-8e79-8ac3241c3d86",
   "metadata": {},
   "source": [
    "## load daily soil temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa918b92-e834-4bc6-acfc-86ad8bf804be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fdir = \"data/Reynolds/soiltemperature\"\n",
    "\n",
    "def loadReynolds(fname):\n",
    "    df = pd.read_csv(fname, delim_whitespace=True,comment=\"#\", encoding=\"ISO-8859-1\",on_bad_lines=\"warn\",header=None)\n",
    "    df = df[df[3]==17]\n",
    "    df_time = pd.to_datetime(df[0].astype(str) +  ' ' + df[1].astype(str) + ' ' + df[2].astype(str),format=\"%m %d %Y\")\n",
    "    \n",
    "    df2 = df[[5,6,7,8,9,10,11,12]].replace('.', np.nan).astype(float).ffill(axis=0)\n",
    "\n",
    "    df2[1] = fname.replace(fdir,\"\").replace(\"/hourly\",\"\").replace(\"soiltemperature.txt\",\"\")\n",
    "    \n",
    "    return pd.concat([df_time,df2], axis=1) \n",
    "\n",
    "soil_temp = []\n",
    "for file in os.listdir(fdir):\n",
    "    if file.endswith(\".txt\") and file.find(\"hourly\") > -1:\n",
    "        print(file)\n",
    "        path = os.path.join(fdir, file)\n",
    "        soil_temp += [loadReynolds(path)]\n",
    "\n",
    "reynolds_soil_temp = pd.concat(soil_temp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d004de7-d9e1-4212-9509-3bf90f285a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir2 = \"data/Reynolds\"\n",
    "\n",
    "def loadReynoldsCL(fname):\n",
    "    df = pd.read_csv(fname, delim_whitespace=True,comment=\"#\", encoding=\"ISO-8859-1\",on_bad_lines=\"warn\",header=None)\n",
    "    df = df[(df[2] > 1984)|(df[2] == 1984) & (df[0] == 12) &(df[1] > 4)]\n",
    "    df_time = pd.to_datetime(df[0].astype(str) +  ' ' + df[1].astype(str) + ' ' + df[2].astype(str),format=\"%m %d %Y\")\n",
    "    \n",
    "    df2 = df[[3,4]].replace('.', np.nan).astype(float).ffill(axis=0)\n",
    "\n",
    "    df2[1] = fname.replace(fdir2,\"\").replace(\"/daily\",\"\").replace(\"climate.txt\",\"\")\n",
    "    \n",
    "    return pd.concat([df_time,df2], axis=1) \n",
    "\n",
    "climate = []\n",
    "for file in os.listdir(fdir2):\n",
    "    if file.endswith(\".txt\") and file.find(\"daily\") > -1:\n",
    "        print(file)\n",
    "        path = os.path.join(fdir2, file)\n",
    "        climate += [loadReynoldsCL(path)]\n",
    "\n",
    "reynolds_climate = pd.concat(climate, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aae251-610a-442a-9b1e-9e452a258382",
   "metadata": {},
   "outputs": [],
   "source": [
    "reynolds_soil_temp = reynolds_soil_temp.rename(columns={0:'DATE',1:\"LOC\",5:\"y_1\",6:\"y_2\",7:\"y_3\",8:\"y_4\",9:\"y_5\",10:\"y_6\",11:\"y_7\",12:\"y_8\",3: 20, 4: 21}) \n",
    "reynolds_climate = reynolds_climate.rename(columns={0:'DATE',1:\"LOC\"})\n",
    "climate = reynolds_climate.set_index(['DATE','LOC']).rename(columns={3: \"C1\", 4: \"C2\"}) \n",
    "soil = reynolds_soil_temp.set_index(['DATE','LOC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cd1dd-af3a-4c24-9075-58caa38f86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reynolds_data = pd.merge(soil,climate,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e69bb-f648-4567-87ef-537d937d4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reynolds_data.to_csv(\"all_reynolds_data.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf921e1-416b-4335-8ab5-0df3e89d570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reynolds_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0ed8d-2138-4411-bd66-eb825c170aec",
   "metadata": {},
   "source": [
    "# UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ed1ef-e310-4ba8-8404-c70579c2af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_data = \"data/UK/catalogue.ceh.ac.uk/datastore/eidchub/399ed9b1-bf59-4d85-9832-ee4d29f49bfb/\"\n",
    "climate_soil = []\n",
    "\n",
    "def loadUK(fname):\n",
    "    df = pd.read_csv(fname, sep=\",\",comment=\"#\", on_bad_lines=\"warn\")\n",
    "\n",
    "    df1 = df.iloc[:, :2]\n",
    "    df2 = df.iloc[:, 2:]    \n",
    "    df2 = df2.astype(float).replace(-9999.0, np.nan).ffill(axis=0)    \n",
    "    \n",
    "    return pd.concat([df1,df2], axis=1)\n",
    "    \n",
    "\n",
    "for file in os.listdir(uk_data):\n",
    "    if file.endswith(\".csv\") and file.find(\"daily\") > -1 and file.find(\"flags\") == -1 and file.find(\"metadata\") == -1:\n",
    "        print(file)\n",
    "        path = os.path.join(uk_data, file)\n",
    "        climate_soil += [loadUK(path)]\n",
    "\n",
    "climate_soil =  pd.concat(climate_soil, axis=0)\n",
    "climate_soil = climate_soil.rename(columns={\"DATE_TIME\":\"DATE\",\"SITE_ID\":\"LOC\",\"TDT1_TSOIL\":\"y_1\",\"TDT2_TSOIL\":\"y_2\",\"TDT3_TSOIL\":\"y_3\",\"TDT4_TSOIL\":\"y_4\",\"TDT5_TSOIL\":\"y_5\",\"TDT6_TSOIL\":\"y_6\",\"TDT7_TSOIL\":\"y_7\",\"TDT8_TSOIL\":\"y_8\",\"TDT9_TSOIL\":\"y_9\",\"TDT10_TSOIL\":\"y_10\"}) \n",
    "climate_soil['DATE']= pd.to_datetime(climate_soil['DATE'])\n",
    "climate_soil.set_index([\"DATE\",\"LOC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e6c7a-5695-479e-aec3-68498a28b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil.rename(columns={\"DATE_TIME\":\"DATE\",\"SITE_ID\":\"LOC\",\"TDT1_TSOIL\":\"y_1\",\"TDT2_TSOIL\":\"y_2\",\"TDT3_TSOIL\":\"y_3\",\"TDT4_TSOIL\":\"y_4\",\"TDT5_TSOIL\":\"y_5\",\"TDT6_TSOIL\":\"y_6\",\"TDT7_TSOIL\":\"y_7\",\"TDT8_TSOIL\":\"y_8\",\"TDT9_TSOIL\":\"y_9\",\"TDT10_TSOIL\":\"y_10\"}) \n",
    "climate_soil = climate_soil.set_index([\"DATE\",\"LOC\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1672478-1885-4806-910f-e80cb44ac9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil.to_csv(\"uk_soil.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36080b6-8644-4a5f-860d-f155803dc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4eaa79-83d9-4c50-87ff-6f8e664ecd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil = climate_soil.drop([\"SNOW_DEPTH\", \"TDT1_VWC\",\"TDT2_VWC\",\"TDT3_VWC\",\"TDT4_VWC\",\"TDT5_VWC\",\"TDT6_VWC\",\"TDT7_VWC\",\"TDT8_VWC\",\"TDT9_VWC\",\"TDT10_VWC\",\"PRECIP_TIPPING\",\"PRECIP_RAINE\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b260c2-cee3-4d1e-bc45-10db2624173d",
   "metadata": {},
   "source": [
    "# Preprocessing of the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf6c33-e6d6-45b7-9da9-7f8d4c1697c0",
   "metadata": {},
   "source": [
    "### Split onto 28 day long fragments. Remove fragments, which do not containt more than 30% of target values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff8e5b-2621-4e73-9b50-6490ac384923",
   "metadata": {},
   "source": [
    "### We do not consider temperatures for other soil layers as fatures deliberatly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384146e-1ecc-4b83-b779-7029e3d4b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "PERIOD = 28\n",
    "TRHX = 0.3\n",
    "\n",
    "def daterange(start_date, end_date, step):\n",
    "    for n in range(0,int((end_date - start_date).days), step):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "all_data = []\n",
    "for df in [all_reynolds_data, climate_soil]:\n",
    "    locations = df.index.get_level_values(1)\n",
    "    \n",
    "    rdatasX = {}\n",
    "    rdatasY = {}\n",
    "    for l in set(locations):\n",
    "        ld = df.query(\"LOC == '\" + l + \"'\").reset_index().set_index(\"DATE\").drop(\"LOC\",axis=1)\n",
    "        all_start = ld.index.min()\n",
    "        all_end = ld.index.max()\n",
    "    \n",
    "        for start in daterange(all_start, all_end, PERIOD):\n",
    "            end = start + timedelta(PERIOD)\n",
    "            period_data = ld.loc[start:end]\n",
    "            \n",
    "            y_columns = set([c for c in df.columns if c.find(\"y_\") > -1])\n",
    "            not_y_columns = [c for c in df.columns if c.find(\"y_\") == -1]\n",
    "            for y_counter in y_columns:\n",
    "                if y_counter not in rdatasX: \n",
    "                    rdatasX[y_counter] = []\n",
    "                    rdatasY[y_counter] = []\n",
    "                    \n",
    "                y_ = period_data[y_counter].to_numpy().astype(float)\n",
    "                y = y_[1:]\n",
    "    \n",
    "                nans = np.count_nonzero(np.isnan(y))\n",
    "    \n",
    "                if float(nans) / y.shape[0] < TRHX and y.shape[0] == PERIOD:\n",
    "                    X = period_data[not_y_columns].to_numpy()[:-1].astype(float)\n",
    "                    X = np.hstack([X,y_[:-1].reshape(-1,1)]) # add current y as a feature\n",
    "                    old_dim = X.shape[1]\n",
    "                    X = imp.fit_transform(X)\n",
    "                    if X.shape[1] == old_dim:\n",
    "                        nans = np.count_nonzero(np.isnan(X))\n",
    "                        if nans == 0:\n",
    "                            y = imp.fit_transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "                            rdatasX[y_counter].append(X)\n",
    "                            rdatasY[y_counter].append(y)\n",
    "                    \n",
    "    all_data.append({k:[np.asarray(rdatasX[k]), np.asarray(rdatasY[k])] for k in rdatasX})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455aaf36-c9e2-4c5c-a864-c5123484fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70c021-80be-439d-9a0e-5f72fe081e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "for k in all_data[0]:\n",
    "    all_data[0][k][0] = normalize(all_data[0][k][0].reshape(-1,all_data[0][k][0].shape[2]),axis=0).reshape(all_data[0][k][0].shape)\n",
    "    \n",
    "for k in all_data[1]:\n",
    "    all_data[1][k][0] = normalize(all_data[1][k][0].reshape(-1,all_data[1][k][0].shape[2]),axis=0).reshape(all_data[1][k][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205d89a-3a6a-4f01-9537-2d70b3cc3018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a529a-614a-49e0-884b-0ab0ae3c6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dict_data = {}\n",
    "dict_data[\"Reynolds\"] = {}\n",
    "dict_data[\"UK\"] = {}\n",
    "\n",
    "for k in all_data[0]:\n",
    "    x01,x02,y01,y02 = train_test_split(all_data[0][k][0], all_data[0][k][1], test_size=0.3,random_state=42)\n",
    "    dict_data[\"Reynolds\"][k] = {\"train\":{\"X\":x01,\"y\":y01},\"test\":{\"X\":x02,\"y\":y02}}\n",
    "    \n",
    "for k in all_data[1]:\n",
    "    x11,x12,y11,y12 = train_test_split(all_data[1][k][0], all_data[1][k][1],test_size=0.3,random_state=42)\n",
    "    dict_data[\"UK\"][k] = {\"train\":{\"X\":x11,\"y\":y11},\"test\":{\"X\":x12,\"y\":y12}} \n",
    "\n",
    "all_data = dict_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220b854-2101-4a5b-8d3a-58256166190b",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149bcd0-33fb-4882-bc91-7e69feb34808",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ac0b2-b494-4f11-9966-6988ed981e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==4.21.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd4688-1e45-4fa8-816a-7c0b9a20621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37d7c5-c860-4144-87be-25432e02a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cab2f4-eef5-4c80-a528-bbe0fa4672d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801dd1a7-1dc2-454b-bf0c-afce7e161d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50015bf6-285c-4b21-91e2-cb3c9ebe7a2c",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ee698-ed63-437e-8051-e9152270dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import optuna\n",
    "from tkan import TKAN, BSplineActivation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "nn_data = []\n",
    "\n",
    "\n",
    "def make_modelLSTM(input_shape, hidden_size, dropout):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "    #dim = tf.zeros([batch_size,hidden_size])  \n",
    "    output_layer = tf.keras.layers.LSTM(hidden_size, return_sequences=True,dropout=dropout)(input_layer)#, initial_state=[dim, dim])\n",
    "    output_layer2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear'))(output_layer)    \n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer2)\n",
    " \n",
    "def make_GRU(input_shape, hidden_size, dropout):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "    output_layer = tf.keras.layers.GRU(hidden_size, return_sequences=True,dropout=dropout)(input_layer)#, initial_state=[dim, dim])\n",
    "    output_layer2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear'))(output_layer)    \n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "\n",
    "def make_modelTKAN(input_shape, hidden_size, dropout):\n",
    "    model = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "          TKAN(hidden_size, tkan_activations=[BSplineActivation(3)], return_sequences=True, use_bias=True, kernel_regularizer=regularizers.L2(dropout),\n",
    "    bias_regularizer=regularizers.L2(dropout), recurrent_regularizer=regularizers.L2(dropout)),\n",
    "          tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear')),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "models = {\"LSTM\":make_modelLSTM, \"GRU\": make_GRU, \"TKAN\": make_modelTKAN}\n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"]\n",
    "            Y_test = dat[\"test\"][\"y\"]\n",
    "            batch_size = 4       \n",
    "    \n",
    "            def objective(trial):\n",
    "                lr = trial.suggest_float('lr', 0.0001, 0.01)\n",
    "                hidden_size = trial.suggest_int('hs', 2, 8)\n",
    "                en = trial.suggest_int('en', 10, 400)\n",
    "\n",
    "                if model_name == \"TKAN\":\n",
    "                    do = trial.suggest_float('dropout', 1e-5, 1e-2)\n",
    "                else:    \n",
    "                    do = trial.suggest_float('dropout', 0.05, 0.2)\n",
    "                    \n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(input_shape=x_train.shape[1:],hidden_size=hidden_size, dropout = do)\n",
    "                # \n",
    "                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                \n",
    "                    model.compile(\n",
    "                         optimizer=opt,\n",
    "                         loss=\"mean_squared_error\",\n",
    "                         metrics=[\"mean_squared_error\"],\n",
    "                    )\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=en,\n",
    "                         verbose=0,\n",
    "                    )\n",
    "                    try:\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                    except:\n",
    "                        scores.append(500)\n",
    "                return np.asarray(scores).mean() \n",
    "                \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=20)    \n",
    "            \n",
    "            lr = study.best_trial.params[\"lr\"]     \n",
    "            hs = study.best_trial.params[\"hs\"]     \n",
    "            en = study.best_trial.params[\"en\"]   \n",
    "            do = study.best_trial.params[\"dropout\"]   \n",
    "    \n",
    "            model = make_model(input_shape=x_train.shape[1:],hidden_size=hs, dropout = do)\n",
    "        # \n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "            model.compile(\n",
    "                 optimizer=opt,\n",
    "                 loss=\"mean_squared_error\",\n",
    "                 metrics=[\"mean_squared_error\"],\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=en,\n",
    "                 verbose=0,\n",
    "            )        \n",
    "            try:\n",
    "                y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "                mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "                mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "                printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"networks_output.txt\")     \n",
    "                nn_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "            except:\n",
    "                pass\n",
    "            del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cc727-6ec0-4692-9292-dc887ea20006",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0c1e4-e919-4263-a024-313b3fc17b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install deep-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e55aa-3f02-4947-8fb0-0afbd49315e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cb9f5-d07a-40da-a96c-fc6ec26e8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875b738-00c9-466d-acb9-02b3cb7c3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelXGB(max_depth,layers,C):\n",
    "    return xgb.XGBRegressor(max_depth = max_depth, n_estimators = layers)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C):\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4)\n",
    "\n",
    "def make_modelBoosted(max_depth,layers,C):\n",
    "    return CascadeBoostingRegressor(C=C, n_layers=layers, n_estimators = 1, max_depth=max_depth, n_iter_no_change = 1, validation_fraction = 0.1, learning_rate = 0.9)\n",
    "\n",
    "\n",
    "models = {\"XGB\":make_modelXGB,\"Cascade Forest\":make_modelCascade, \"Boosted Forest\": make_modelBoosted}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])\n",
    "            x_test = dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2])\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 1, 2000)\n",
    "                else:\n",
    "                    C = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(max_depth,layers,C)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=20)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]  \n",
    "            else:\n",
    "                C = 0\n",
    "            model = make_model(max_depth,layers,C)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"boosting_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765618c5-d00f-416e-b021-1db9a5bde06d",
   "metadata": {},
   "source": [
    "# Adaptive weighing (AWDF and ECDFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58e59e-ff90-4f6b-8ce6-3838d291645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECDFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1ef4c-4c9a-489c-a42d-72cb00ab22fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import optuna\n",
    "\n",
    "from ecdfr.gcForest import gcForest\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelECDFR(max_depth,layers,resampling_rate, et):\n",
    "    config = {\"estimator_configs\":[{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth},{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth},{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth},{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth}],\n",
    "              \"error_threshold\": et,\n",
    "              \"resampling_rate\": resampling_rate,\n",
    "              \"random_state\":None,\n",
    "              \"max_layers\":layers,\n",
    "              \"early_stop_rounds\":1,\n",
    "              \"train_evaluation\":r2_score}\n",
    "    \n",
    "    return gcForest(config,2)\n",
    "\n",
    "models = {\"ecdfr\":make_modelECDFR}\n",
    "\n",
    "bo_data = []    \n",
    "work_pair = []\n",
    "best_pair = []\n",
    "\n",
    "max_score = 100\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])\n",
    "            x_test = dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2])\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                global max_score\n",
    "                layers = trial.suggest_int('layers', 3, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                C = trial.suggest_float('resampling_rate', 0.1, 4)\n",
    "                min_et = 0.5 * C - 1\n",
    "                if min_et <= 0:\n",
    "                    min_et = 0.05\n",
    "                else:\n",
    "                    if min_et > 0.99:\n",
    "                        min_et = 0.99\n",
    "\n",
    "                max_et = C -1.\n",
    "\n",
    "                if max_et > 0.99:\n",
    "                    max_et = 0.99\n",
    "                else:\n",
    "                    if max_et <=0:\n",
    "                        max_et = 0.1\n",
    "\n",
    "                if min_et >= max_et:\n",
    "                    max_et = min_et + 0.001\n",
    "                    \n",
    "                et = trial.suggest_float('et', 0.05, 0.95)\n",
    "                \n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                try:\n",
    "                    for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                        model = make_modelECDFR(max_depth,layers,C,et)\n",
    "                    \n",
    "                        model.fit(\n",
    "                             x_train[train_index],\n",
    "                             Y_train[train_index],\n",
    "                        )\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                        sc = mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten())\n",
    "                        if max_score == 100:\n",
    "                            max_score = sc\n",
    "                        else:\n",
    "                            if sc > max_score:\n",
    "                                max_score = sc\n",
    "                                \n",
    "                        work_pair.append([C,et])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    scores = [max_score]\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=100)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "\n",
    "            C = study.best_trial.params[\"resampling_rate\"]  \n",
    "            et = study.best_trial.params[\"et\"]  \n",
    "\n",
    "            best_pair.append([C,et])\n",
    "            model = make_model(max_depth,layers,C,et)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"ecdfr_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832896df-6af8-4b2b-806e-8dbb5c210636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(*zip(*work_pair))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f69de-e579-4cb7-9ec7-1a1d689279ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75081d0-28a8-4abc-9e42-bdd8f859cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028cdcb-1534-4ca0-940e-97f6bd94dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(*zip(*best_pair))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e308d-25b4-44e3-8750-e3d2bfc058f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C,wt):\n",
    "    wf = {0:\"linear\", 1:\"1-w^1/2\", 2:\"1-w2\"}\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4,adaptive=True,weighting_function = wf[wt],verbose=0,trx=1.0)\n",
    "\n",
    "\n",
    "models = {\"AWDF\":make_modelCascade}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])\n",
    "            x_test = dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2])\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "                wt = trial.suggest_int('weight_function', 0, 2)   \n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 1, 2000)\n",
    "                else:\n",
    "                    C = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(max_depth,layers,C,wt)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict_sampled(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=100)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "            wt = study.best_trial.params['weight_function']\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]  \n",
    "            else:\n",
    "                C = 0\n",
    "            model = make_model(max_depth,layers,C,wt)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict_sampled(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"awdf_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869dd8a-6ea3-4538-b81f-0ae41b97d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy==1.22.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffd464-2b3e-4503-84a8-87817cd2e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b679529-47d3-4cef-bf4e-ecd44c7f6bd7",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f10705-60f6-4248-baa1-bc68bdd4e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelARIMA(p,d,q,y):\n",
    "    return SARIMAX(y.flatten(), order=(p,d,q),seasonal_order=(1,0,1,7),trend='c')\n",
    "\n",
    "\n",
    "models = {\"ARIMA\": make_modelARIMA}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"]\n",
    "            Y_test = dat[\"test\"][\"y\"]\n",
    "\n",
    "            print(Y_train.shape,Y_test.shape)\n",
    "\n",
    "            def objective(trial):\n",
    "                p = trial.suggest_int('p', 1, 5)\n",
    "                d = trial.suggest_int('d', 0, 5)\n",
    "                q = trial.suggest_int('q', 0, 5)\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                try:\n",
    "                    for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                        for y_t in Y_train[train_index]:\n",
    "                            model = make_model(p,d,q, y_t[:27].flatten())\n",
    "                            res = model.fit() \n",
    "                            y_pred = res.forecast(steps = 1) #, batch_size=batch_size)\n",
    "                            \n",
    "                            scores.append(mean_squared_error(np.asarray([y_t[27]]),y_pred))\n",
    "                except:\n",
    "                    scores.append(1000)\n",
    "\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=100)    \n",
    "\n",
    "            try:\n",
    "                p = study.best_trial.params[\"p\"]  \n",
    "                d = study.best_trial.params[\"d\"]  \n",
    "                q = study.best_trial.params[\"q\"]  \n",
    "\n",
    "                mse_list = []\n",
    "                mae_list = []\n",
    "                \n",
    "                for y_t in Y_test:\n",
    "                    model = make_model(p,d,q,y_t[:27])\n",
    "                    res = model.fit() \n",
    "                    y_pred = res.forecast(steps = 1) #, batch_size=batch_size)\n",
    "                    mse_list.append(mean_squared_error(np.asarray([y_t[27]]),y_pred))\n",
    "                    mae_list.append(mean_absolute_error(np.asarray([y_t[27]]),y_pred))\n",
    "                    \n",
    "                mse_score = np.asarray(mse_list).mean()\n",
    "                mae_score = np.asarray(mae_list).mean()\n",
    "                printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),\"ARIMA_output.txt\")     \n",
    "                bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "\n",
    "#print (model.mle_retvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3441aa5-1ed2-4a2d-a015-ce7ba440e584",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517ca83-e93c-430f-b746-3267b2c14792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "rf_data = [] \n",
    "\n",
    "for ds_name in all_data:\n",
    "    for depth in all_data[ds_name]:\n",
    "        best = 1000\n",
    "        best_d = 2\n",
    "        for n_estimators in [100]:\n",
    "            for tdepth in [2,3,5,7,10,12]:\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = RandomForestRegressor(n_estimators = n_estimators, max_depth = tdepth)\n",
    "                    dat = all_data[ds_name][depth]\n",
    "                    model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])[train_index], dat[\"train\"][\"y\"].flatten()[train_index])\n",
    "                    y_pred = model.predict(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]))[test_index])\n",
    "                    scores.append(mean_squared_error(y_pred, dat[\"train\"][\"y\"].flatten())[test_index])\n",
    "                    \n",
    "                if np.asarray(scores).mean() < best:\n",
    "                    best = np.asarray(scores).mean()\n",
    "                    best_d = tdepth\n",
    "\n",
    "        model = RandomForestRegressor(n_estimators = n_estimators, max_depth = best_d)\n",
    "        dat = all_data[ds_name][depth]\n",
    "        model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]), dat[\"train\"][\"y\"].flatten())\n",
    "        y_pred = model.predict(dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2]))\n",
    "        mse_score = mean_squared_error(y_pred, dat[\"test\"][\"y\"].flatten())\n",
    "        mae_score = mean_absolute_error(y_pred, dat[\"test\"][\"y\"].flatten())        \n",
    "                \n",
    "        print(\"Random Forest\", ds_name,depth,mse_score, mae_score, dat[\"test\"][\"y\"].min(),dat[\"test\"][\"y\"].max())\n",
    "        rf_data.append([\"Random Forest\", ds_name,depth,mse_score, mae_score])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
