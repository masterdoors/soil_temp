{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1e02a-e59e-4095-ae53-f13e6ee96ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c8e03-889b-4a36-96ba-ce156aa2cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324622c3-29c6-4eae-b94d-b521f2a55961",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8bb49-5d14-425d-a85a-11daddd678d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc0415-9dbc-486f-8ae0-f1598e0132b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10813341-60ea-45d8-be93-2fbf0a81212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "def printf(*args, fname=\"log.txt\"):\n",
    "    with open(join(\"test_outputs\",fname),\"a+\") as f:\n",
    "        for a in args:\n",
    "            f.write(str(a) + \" \")\n",
    "        f.write(\"\\n\") \n",
    "    print(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280cc48-46c8-478b-b6f6-cf7d35431001",
   "metadata": {},
   "source": [
    "# Load Reynolds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7d689-cbb3-45b1-8e79-8ac3241c3d86",
   "metadata": {},
   "source": [
    "## load daily soil temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa918b92-e834-4bc6-acfc-86ad8bf804be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hourly127x07soiltemperature.txt\n",
      "hourly098x97soiltemperature.txt\n",
      "hourly076x59soiltemperature.txt\n",
      "hourly057x96soiltemperature.txt\n",
      "hourly176x14soiltemperature.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "fdir = \"data/Reynolds/soiltemperature\"\n",
    "\n",
    "def loadReynolds(fname):\n",
    "    df = pd.read_csv(fname, delim_whitespace=True,comment=\"#\", encoding=\"ISO-8859-1\",on_bad_lines=\"warn\",header=None)\n",
    "    df = df[df[3]==17]\n",
    "    df_time = pd.to_datetime(df[0].astype(str) +  ' ' + df[1].astype(str) + ' ' + df[2].astype(str),format=\"%m %d %Y\")\n",
    "    \n",
    "    df2 = df[[5,6,7,8,9,10,11,12]].replace('.', np.nan).astype(float).ffill(axis=0)\n",
    "\n",
    "    df2[1] = fname.replace(fdir,\"\").replace(\"/hourly\",\"\").replace(\"soiltemperature.txt\",\"\")\n",
    "    \n",
    "    return pd.concat([df_time,df2], axis=1) \n",
    "\n",
    "soil_temp = []\n",
    "for file in os.listdir(fdir):\n",
    "    if file.endswith(\".txt\") and file.find(\"hourly\") > -1:\n",
    "        print(file)\n",
    "        path = os.path.join(fdir, file)\n",
    "        soil_temp += [loadReynolds(path)]\n",
    "\n",
    "reynolds_soil_temp = pd.concat(soil_temp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d004de7-d9e1-4212-9509-3bf90f285a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily176x14climate.txt\n",
      "daily127x07climate.txt\n",
      "daily076x59climate.txt\n"
     ]
    }
   ],
   "source": [
    "fdir2 = \"data/Reynolds\"\n",
    "\n",
    "def loadReynoldsCL(fname):\n",
    "    df = pd.read_csv(fname, delim_whitespace=True,comment=\"#\", encoding=\"ISO-8859-1\",on_bad_lines=\"warn\",header=None)\n",
    "    df = df[(df[2] > 1984)|(df[2] == 1984) & (df[0] == 12) &(df[1] > 4)]\n",
    "    df_time = pd.to_datetime(df[0].astype(str) +  ' ' + df[1].astype(str) + ' ' + df[2].astype(str),format=\"%m %d %Y\")\n",
    "    \n",
    "    df2 = df[[3,4]].replace('.', np.nan).astype(float).ffill(axis=0)\n",
    "\n",
    "    df2[1] = fname.replace(fdir2,\"\").replace(\"/daily\",\"\").replace(\"climate.txt\",\"\")\n",
    "    \n",
    "    return pd.concat([df_time,df2], axis=1) \n",
    "\n",
    "climate = []\n",
    "for file in os.listdir(fdir2):\n",
    "    if file.endswith(\".txt\") and file.find(\"daily\") > -1:\n",
    "        print(file)\n",
    "        path = os.path.join(fdir2, file)\n",
    "        climate += [loadReynoldsCL(path)]\n",
    "\n",
    "reynolds_climate = pd.concat(climate, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9aae251-610a-442a-9b1e-9e452a258382",
   "metadata": {},
   "outputs": [],
   "source": [
    "reynolds_soil_temp = reynolds_soil_temp.rename(columns={0:'DATE',1:\"LOC\",5:\"y_1\",6:\"y_2\",7:\"y_3\",8:\"y_4\",9:\"y_5\",10:\"y_6\",11:\"y_7\",12:\"y_8\",3: 20, 4: 21}) \n",
    "reynolds_climate = reynolds_climate.rename(columns={0:'DATE',1:\"LOC\"})\n",
    "climate = reynolds_climate.set_index(['DATE','LOC']).rename(columns={3: \"C1\", 4: \"C2\"}) \n",
    "soil = reynolds_soil_temp.set_index(['DATE','LOC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824cd1dd-af3a-4c24-9075-58caa38f86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reynolds_data = pd.merge(soil,climate,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5e69bb-f648-4567-87ef-537d937d4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reynolds_data.to_csv(\"all_reynolds_data.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf921e1-416b-4335-8ab5-0df3e89d570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "      <th>y_4</th>\n",
       "      <th>y_5</th>\n",
       "      <th>y_6</th>\n",
       "      <th>y_7</th>\n",
       "      <th>y_8</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th>LOC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1984-12-05</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.8</td>\n",
       "      <td>-2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-06</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.4</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-07</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-08</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-12-09</th>\n",
       "      <th>127x07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-26</th>\n",
       "      <th>176x14</th>\n",
       "      <td>15.4</td>\n",
       "      <td>12.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-27</th>\n",
       "      <th>176x14</th>\n",
       "      <td>18.4</td>\n",
       "      <td>14.8</td>\n",
       "      <td>10.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-28</th>\n",
       "      <th>176x14</th>\n",
       "      <td>19.9</td>\n",
       "      <td>16.6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>10.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-29</th>\n",
       "      <th>176x14</th>\n",
       "      <td>20.5</td>\n",
       "      <td>17.6</td>\n",
       "      <td>13.1</td>\n",
       "      <td>12.8</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-09-30</th>\n",
       "      <th>176x14</th>\n",
       "      <td>20.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>13.2</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9969 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    y_1   y_2   y_3   y_4   y_5   y_6   y_7   y_8    C1    C2\n",
       "DATE       LOC                                                               \n",
       "1984-12-05 127x07   NaN   0.4   NaN   2.0   NaN   NaN   NaN   NaN  -9.8  -2.4\n",
       "1984-12-06 127x07   NaN   0.3   NaN   1.9   NaN   NaN   NaN   NaN  -9.4  -1.0\n",
       "1984-12-07 127x07   NaN   0.2   NaN   1.8   NaN   NaN   NaN   NaN  -5.2   3.6\n",
       "1984-12-08 127x07   NaN   0.1   NaN   1.7   NaN   NaN   NaN   NaN  -3.6   4.0\n",
       "1984-12-09 127x07   NaN   0.2   NaN   1.6   NaN   NaN   NaN   NaN  -0.1   5.1\n",
       "...                 ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "1996-09-26 176x14  15.4  12.5   9.5   9.6  10.2  10.4  10.8  11.0  -1.1   9.6\n",
       "1996-09-27 176x14  18.4  14.8  10.3  10.2  10.2  10.4  10.7  10.9   1.9  17.0\n",
       "1996-09-28 176x14  19.9  16.6  12.0  11.7  11.0  11.0  10.9  10.9  10.1  21.2\n",
       "1996-09-29 176x14  20.5  17.6  13.1  12.8  11.9  11.7  11.4  11.0  12.4  21.9\n",
       "1996-09-30 176x14  20.4  17.6  13.5  13.2  12.4  12.2  11.9  11.2  13.2  21.8\n",
       "\n",
       "[9969 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reynolds_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0ed8d-2138-4411-bd66-eb825c170aec",
   "metadata": {},
   "source": [
    "# UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52ed1ef-e310-4ba8-8404-c70579c2af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosmos-uk_alic1_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_wyth1_hydrosoil_daily_2013-2023.csv\n",
      "cosmos-uk_sydlg_hydrosoil_daily_2018-2023.csv\n",
      "cosmos-uk_gisbn_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_riseh_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_bickl_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_morem_hydrosoil_daily_2018-2023.csv\n",
      "cosmos-uk_portn_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_stght_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_glenw_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_tadhm_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_wimpl_hydrosoil_daily_2019-2023.csv\n",
      "cosmos-uk_nwyke_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_fivet_hydrosoil_daily_2018-2023.csv\n",
      "cosmos-uk_hlacy_hydrosoil_daily_2018-2023.csv\n",
      "cosmos-uk_plynl_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_glens_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_eastb_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_holln_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_finch_hydrosoil_daily_2017-2023.csv\n",
      "cosmos-uk_sourh_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_lodtn_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_morly_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_elmst_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_hadlw_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_redhl_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_stips_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_rdmer_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_chobh_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_harwd_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_moorh_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_lizrd_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_balrd_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_crich_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_cochn_hydrosoil_daily_2017-2023.csv\n",
      "cosmos-uk_henfs_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_eustn_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_hartw_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_hillb_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_hybry_hydrosoil_daily_2017-2023.csv\n",
      "cosmos-uk_rothd_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_chimn_hydrosoil_daily_2013-2023.csv\n",
      "cosmos-uk_spenf_hydrosoil_daily_2016-2023.csv\n",
      "cosmos-uk_cardt_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_sheep_hydrosoil_daily_2013-2023.csv\n",
      "cosmos-uk_wrttl_hydrosoil_daily_2017-2023.csv\n",
      "cosmos-uk_coclp_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_bunny_hydrosoil_daily_2015-2023.csv\n",
      "cosmos-uk_waddn_hydrosoil_daily_2013-2023.csv\n",
      "cosmos-uk_lulln_hydrosoil_daily_2014-2023.csv\n",
      "cosmos-uk_cgarw_hydrosoil_daily_2016-2023.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LWIN</th>\n",
       "      <th>LWOUT</th>\n",
       "      <th>SWIN</th>\n",
       "      <th>SWOUT</th>\n",
       "      <th>RN</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>PRECIP_TIPPING</th>\n",
       "      <th>PRECIP_RAINE</th>\n",
       "      <th>PA</th>\n",
       "      <th>TA</th>\n",
       "      <th>...</th>\n",
       "      <th>STP_TSOIL50</th>\n",
       "      <th>COSMOS_VWC</th>\n",
       "      <th>CTS_MOD_CORR</th>\n",
       "      <th>D86_75M</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNOW_DEPTH</th>\n",
       "      <th>SWE</th>\n",
       "      <th>ALBEDO</th>\n",
       "      <th>PE</th>\n",
       "      <th>GCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th>LOC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-03-06</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-07</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.7</td>\n",
       "      <td>30.5</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>41.2</td>\n",
       "      <td>1127.86170</td>\n",
       "      <td>22.12836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.126</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-08</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>27.1</td>\n",
       "      <td>30.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1010.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7</td>\n",
       "      <td>43.2</td>\n",
       "      <td>1118.28578</td>\n",
       "      <td>21.68494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-09</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>28.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1011.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>39.1</td>\n",
       "      <td>1138.72068</td>\n",
       "      <td>22.63179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-10</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.5</td>\n",
       "      <td>29.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1016.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1105.15844</td>\n",
       "      <td>21.05842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>29.8</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>38.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>964.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>66.2</td>\n",
       "      <td>1419.61285</td>\n",
       "      <td>16.04571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>28.6</td>\n",
       "      <td>30.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>968.1</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1462.74408</td>\n",
       "      <td>17.23641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>25.6</td>\n",
       "      <td>29.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>973.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>1485.48209</td>\n",
       "      <td>17.86281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>29.7</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>33.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>963.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8</td>\n",
       "      <td>60.2</td>\n",
       "      <td>1441.08650</td>\n",
       "      <td>16.63789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>28.5</td>\n",
       "      <td>29.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>953.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>8.7</td>\n",
       "      <td>55.2</td>\n",
       "      <td>1462.11789</td>\n",
       "      <td>17.21169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154269 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  LWIN  LWOUT  SWIN  SWOUT   RN  PRECIP  PRECIP_TIPPING  \\\n",
       "DATE       LOC                                                            \n",
       "2015-03-06 ALIC1   NaN    NaN   NaN    NaN  NaN     NaN             NaN   \n",
       "2015-03-07 ALIC1  23.7   30.5  13.3    1.9  4.6     0.0             NaN   \n",
       "2015-03-08 ALIC1  27.1   30.5   4.8    0.7  0.7     0.0             NaN   \n",
       "2015-03-09 ALIC1  28.3   29.7   2.4    0.4  0.7     0.1             NaN   \n",
       "2015-03-10 ALIC1  23.5   29.8  12.4    1.8  4.3     0.0             NaN   \n",
       "...                ...    ...   ...    ...  ...     ...             ...   \n",
       "2023-12-27 CGARW  29.8   30.8   0.6    0.2 -0.6    38.7             NaN   \n",
       "2023-12-28 CGARW  28.6   30.5   1.0    0.3 -1.1    10.2             NaN   \n",
       "2023-12-29 CGARW  25.6   29.4   2.8    0.9 -1.9     3.9             NaN   \n",
       "2023-12-30 CGARW  29.7   29.9   0.2    0.1 -0.1    33.6             NaN   \n",
       "2023-12-31 CGARW  28.5   29.7   1.9    0.5  0.2     6.6             NaN   \n",
       "\n",
       "                  PRECIP_RAINE      PA   TA  ...  STP_TSOIL50  COSMOS_VWC  \\\n",
       "DATE       LOC                               ...                            \n",
       "2015-03-06 ALIC1           NaN     NaN  NaN  ...          NaN         NaN   \n",
       "2015-03-07 ALIC1           NaN  1012.2  8.6  ...          5.6        41.2   \n",
       "2015-03-08 ALIC1           NaN  1010.3  8.4  ...          5.7        43.2   \n",
       "2015-03-09 ALIC1           NaN  1011.8  6.2  ...          5.9        39.1   \n",
       "2015-03-10 ALIC1           NaN  1016.6  7.2  ...          5.9        46.3   \n",
       "...                        ...     ...  ...  ...          ...         ...   \n",
       "2023-12-27 CGARW           NaN   964.9  8.8  ...          9.1        66.2   \n",
       "2023-12-28 CGARW           NaN   968.1  8.4  ...          9.1        55.0   \n",
       "2023-12-29 CGARW           NaN   973.1  6.3  ...          9.0        50.3   \n",
       "2023-12-30 CGARW           NaN   963.6  6.4  ...          8.8        60.2   \n",
       "2023-12-31 CGARW           NaN   953.8  6.2  ...          8.7        55.2   \n",
       "\n",
       "                  CTS_MOD_CORR   D86_75M  SNOW  SNOW_DEPTH      SWE  ALBEDO  \\\n",
       "DATE       LOC                                                                \n",
       "2015-03-06 ALIC1           NaN       NaN   NaN         NaN      NaN     NaN   \n",
       "2015-03-07 ALIC1    1127.86170  22.12836   0.0         NaN      NaN   0.126   \n",
       "2015-03-08 ALIC1    1118.28578  21.68494   0.0         NaN      NaN   0.116   \n",
       "2015-03-09 ALIC1    1138.72068  22.63179   0.0         NaN      NaN   0.111   \n",
       "2015-03-10 ALIC1    1105.15844  21.05842   0.0         NaN      NaN   0.121   \n",
       "...                        ...       ...   ...         ...      ...     ...   \n",
       "2023-12-27 CGARW    1419.61285  16.04571   0.0         0.1  1.81413   0.153   \n",
       "2023-12-28 CGARW    1462.74408  17.23641   0.0         0.1  1.81413   0.238   \n",
       "2023-12-29 CGARW    1485.48209  17.86281   0.0         0.1  1.81413   0.274   \n",
       "2023-12-30 CGARW    1441.08650  16.63789   0.0         0.1  1.81413   0.166   \n",
       "2023-12-31 CGARW    1462.11789  17.21169   0.0         0.1  1.81413   0.229   \n",
       "\n",
       "                   PE  GCC  \n",
       "DATE       LOC              \n",
       "2015-03-06 ALIC1  NaN  NaN  \n",
       "2015-03-07 ALIC1  1.7  NaN  \n",
       "2015-03-08 ALIC1  0.6  NaN  \n",
       "2015-03-09 ALIC1  0.3  NaN  \n",
       "2015-03-10 ALIC1  1.6  NaN  \n",
       "...               ...  ...  \n",
       "2023-12-27 CGARW  0.3  NaN  \n",
       "2023-12-28 CGARW  0.8  NaN  \n",
       "2023-12-29 CGARW  0.8  NaN  \n",
       "2023-12-30 CGARW  0.1  NaN  \n",
       "2023-12-31 CGARW  0.6  NaN  \n",
       "\n",
       "[154269 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_data = \"data/UK/catalogue.ceh.ac.uk/datastore/eidchub/399ed9b1-bf59-4d85-9832-ee4d29f49bfb/\"\n",
    "climate_soil = []\n",
    "\n",
    "def loadUK(fname):\n",
    "    df = pd.read_csv(fname, sep=\",\",comment=\"#\", on_bad_lines=\"warn\")\n",
    "\n",
    "    df1 = df.iloc[:, :2]\n",
    "    df2 = df.iloc[:, 2:]    \n",
    "    df2 = df2.astype(float).replace(-9999.0, np.nan).ffill(axis=0)    \n",
    "    \n",
    "    return pd.concat([df1,df2], axis=1)\n",
    "    \n",
    "\n",
    "for file in os.listdir(uk_data):\n",
    "    if file.endswith(\".csv\") and file.find(\"daily\") > -1 and file.find(\"flags\") == -1 and file.find(\"metadata\") == -1:\n",
    "        print(file)\n",
    "        path = os.path.join(uk_data, file)\n",
    "        climate_soil += [loadUK(path)]\n",
    "\n",
    "climate_soil =  pd.concat(climate_soil, axis=0)\n",
    "climate_soil = climate_soil.rename(columns={\"DATE_TIME\":\"DATE\",\"SITE_ID\":\"LOC\",\"TDT1_TSOIL\":\"y_1\",\"TDT2_TSOIL\":\"y_2\",\"TDT3_TSOIL\":\"y_3\",\"TDT4_TSOIL\":\"y_4\",\"TDT5_TSOIL\":\"y_5\",\"TDT6_TSOIL\":\"y_6\",\"TDT7_TSOIL\":\"y_7\",\"TDT8_TSOIL\":\"y_8\",\"TDT9_TSOIL\":\"y_9\",\"TDT10_TSOIL\":\"y_10\"}) \n",
    "climate_soil['DATE']= pd.to_datetime(climate_soil['DATE'])\n",
    "climate_soil.set_index([\"DATE\",\"LOC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82e6c7a-5695-479e-aec3-68498a28b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil.rename(columns={\"DATE_TIME\":\"DATE\",\"SITE_ID\":\"LOC\",\"TDT1_TSOIL\":\"y_1\",\"TDT2_TSOIL\":\"y_2\",\"TDT3_TSOIL\":\"y_3\",\"TDT4_TSOIL\":\"y_4\",\"TDT5_TSOIL\":\"y_5\",\"TDT6_TSOIL\":\"y_6\",\"TDT7_TSOIL\":\"y_7\",\"TDT8_TSOIL\":\"y_8\",\"TDT9_TSOIL\":\"y_9\",\"TDT10_TSOIL\":\"y_10\"}) \n",
    "climate_soil = climate_soil.set_index([\"DATE\",\"LOC\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1672478-1885-4806-910f-e80cb44ac9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil.to_csv(\"uk_soil.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36080b6-8644-4a5f-860d-f155803dc18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LWIN</th>\n",
       "      <th>LWOUT</th>\n",
       "      <th>SWIN</th>\n",
       "      <th>SWOUT</th>\n",
       "      <th>RN</th>\n",
       "      <th>PRECIP</th>\n",
       "      <th>PRECIP_TIPPING</th>\n",
       "      <th>PRECIP_RAINE</th>\n",
       "      <th>PA</th>\n",
       "      <th>TA</th>\n",
       "      <th>...</th>\n",
       "      <th>STP_TSOIL50</th>\n",
       "      <th>COSMOS_VWC</th>\n",
       "      <th>CTS_MOD_CORR</th>\n",
       "      <th>D86_75M</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNOW_DEPTH</th>\n",
       "      <th>SWE</th>\n",
       "      <th>ALBEDO</th>\n",
       "      <th>PE</th>\n",
       "      <th>GCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th>LOC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-03-06</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-07</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.7</td>\n",
       "      <td>30.5</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>41.2</td>\n",
       "      <td>1127.86170</td>\n",
       "      <td>22.12836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.126</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-08</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>27.1</td>\n",
       "      <td>30.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1010.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7</td>\n",
       "      <td>43.2</td>\n",
       "      <td>1118.28578</td>\n",
       "      <td>21.68494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-09</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>28.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1011.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>39.1</td>\n",
       "      <td>1138.72068</td>\n",
       "      <td>22.63179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-10</th>\n",
       "      <th>ALIC1</th>\n",
       "      <td>23.5</td>\n",
       "      <td>29.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1016.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1105.15844</td>\n",
       "      <td>21.05842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>29.8</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>38.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>964.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>66.2</td>\n",
       "      <td>1419.61285</td>\n",
       "      <td>16.04571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>28.6</td>\n",
       "      <td>30.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>968.1</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1462.74408</td>\n",
       "      <td>17.23641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>25.6</td>\n",
       "      <td>29.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>973.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>1485.48209</td>\n",
       "      <td>17.86281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>29.7</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>33.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>963.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8</td>\n",
       "      <td>60.2</td>\n",
       "      <td>1441.08650</td>\n",
       "      <td>16.63789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <th>CGARW</th>\n",
       "      <td>28.5</td>\n",
       "      <td>29.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>953.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>8.7</td>\n",
       "      <td>55.2</td>\n",
       "      <td>1462.11789</td>\n",
       "      <td>17.21169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.81413</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154269 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  LWIN  LWOUT  SWIN  SWOUT   RN  PRECIP  PRECIP_TIPPING  \\\n",
       "DATE       LOC                                                            \n",
       "2015-03-06 ALIC1   NaN    NaN   NaN    NaN  NaN     NaN             NaN   \n",
       "2015-03-07 ALIC1  23.7   30.5  13.3    1.9  4.6     0.0             NaN   \n",
       "2015-03-08 ALIC1  27.1   30.5   4.8    0.7  0.7     0.0             NaN   \n",
       "2015-03-09 ALIC1  28.3   29.7   2.4    0.4  0.7     0.1             NaN   \n",
       "2015-03-10 ALIC1  23.5   29.8  12.4    1.8  4.3     0.0             NaN   \n",
       "...                ...    ...   ...    ...  ...     ...             ...   \n",
       "2023-12-27 CGARW  29.8   30.8   0.6    0.2 -0.6    38.7             NaN   \n",
       "2023-12-28 CGARW  28.6   30.5   1.0    0.3 -1.1    10.2             NaN   \n",
       "2023-12-29 CGARW  25.6   29.4   2.8    0.9 -1.9     3.9             NaN   \n",
       "2023-12-30 CGARW  29.7   29.9   0.2    0.1 -0.1    33.6             NaN   \n",
       "2023-12-31 CGARW  28.5   29.7   1.9    0.5  0.2     6.6             NaN   \n",
       "\n",
       "                  PRECIP_RAINE      PA   TA  ...  STP_TSOIL50  COSMOS_VWC  \\\n",
       "DATE       LOC                               ...                            \n",
       "2015-03-06 ALIC1           NaN     NaN  NaN  ...          NaN         NaN   \n",
       "2015-03-07 ALIC1           NaN  1012.2  8.6  ...          5.6        41.2   \n",
       "2015-03-08 ALIC1           NaN  1010.3  8.4  ...          5.7        43.2   \n",
       "2015-03-09 ALIC1           NaN  1011.8  6.2  ...          5.9        39.1   \n",
       "2015-03-10 ALIC1           NaN  1016.6  7.2  ...          5.9        46.3   \n",
       "...                        ...     ...  ...  ...          ...         ...   \n",
       "2023-12-27 CGARW           NaN   964.9  8.8  ...          9.1        66.2   \n",
       "2023-12-28 CGARW           NaN   968.1  8.4  ...          9.1        55.0   \n",
       "2023-12-29 CGARW           NaN   973.1  6.3  ...          9.0        50.3   \n",
       "2023-12-30 CGARW           NaN   963.6  6.4  ...          8.8        60.2   \n",
       "2023-12-31 CGARW           NaN   953.8  6.2  ...          8.7        55.2   \n",
       "\n",
       "                  CTS_MOD_CORR   D86_75M  SNOW  SNOW_DEPTH      SWE  ALBEDO  \\\n",
       "DATE       LOC                                                                \n",
       "2015-03-06 ALIC1           NaN       NaN   NaN         NaN      NaN     NaN   \n",
       "2015-03-07 ALIC1    1127.86170  22.12836   0.0         NaN      NaN   0.126   \n",
       "2015-03-08 ALIC1    1118.28578  21.68494   0.0         NaN      NaN   0.116   \n",
       "2015-03-09 ALIC1    1138.72068  22.63179   0.0         NaN      NaN   0.111   \n",
       "2015-03-10 ALIC1    1105.15844  21.05842   0.0         NaN      NaN   0.121   \n",
       "...                        ...       ...   ...         ...      ...     ...   \n",
       "2023-12-27 CGARW    1419.61285  16.04571   0.0         0.1  1.81413   0.153   \n",
       "2023-12-28 CGARW    1462.74408  17.23641   0.0         0.1  1.81413   0.238   \n",
       "2023-12-29 CGARW    1485.48209  17.86281   0.0         0.1  1.81413   0.274   \n",
       "2023-12-30 CGARW    1441.08650  16.63789   0.0         0.1  1.81413   0.166   \n",
       "2023-12-31 CGARW    1462.11789  17.21169   0.0         0.1  1.81413   0.229   \n",
       "\n",
       "                   PE  GCC  \n",
       "DATE       LOC              \n",
       "2015-03-06 ALIC1  NaN  NaN  \n",
       "2015-03-07 ALIC1  1.7  NaN  \n",
       "2015-03-08 ALIC1  0.6  NaN  \n",
       "2015-03-09 ALIC1  0.3  NaN  \n",
       "2015-03-10 ALIC1  1.6  NaN  \n",
       "...               ...  ...  \n",
       "2023-12-27 CGARW  0.3  NaN  \n",
       "2023-12-28 CGARW  0.8  NaN  \n",
       "2023-12-29 CGARW  0.8  NaN  \n",
       "2023-12-30 CGARW  0.1  NaN  \n",
       "2023-12-31 CGARW  0.6  NaN  \n",
       "\n",
       "[154269 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4eaa79-83d9-4c50-87ff-6f8e664ecd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_soil = climate_soil.drop([\"SNOW_DEPTH\", \"TDT1_VWC\",\"TDT2_VWC\",\"TDT3_VWC\",\"TDT4_VWC\",\"TDT5_VWC\",\"TDT6_VWC\",\"TDT7_VWC\",\"TDT8_VWC\",\"TDT9_VWC\",\"TDT10_VWC\",\"PRECIP_TIPPING\",\"PRECIP_RAINE\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b260c2-cee3-4d1e-bc45-10db2624173d",
   "metadata": {},
   "source": [
    "# Preprocessing of the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf6c33-e6d6-45b7-9da9-7f8d4c1697c0",
   "metadata": {},
   "source": [
    "### Split onto 28 day long fragments. Remove fragments, which do not containt more than 30% of target values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff8e5b-2621-4e73-9b50-6490ac384923",
   "metadata": {},
   "source": [
    "### We do not consider temperatures for other soil layers as fatures deliberatly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6384146e-1ecc-4b83-b779-7029e3d4b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "PERIOD = 28\n",
    "TRHX = 0.3\n",
    "\n",
    "def daterange(start_date, end_date, step):\n",
    "    for n in range(0,int((end_date - start_date).days), step):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "all_data = []\n",
    "for df in [all_reynolds_data, climate_soil]:\n",
    "    locations = df.index.get_level_values(1)\n",
    "    \n",
    "    rdatasX = {}\n",
    "    rdatasY = {}\n",
    "    for l in set(locations):\n",
    "        ld = df.query(\"LOC == '\" + l + \"'\").reset_index().set_index(\"DATE\").drop(\"LOC\",axis=1)\n",
    "        all_start = ld.index.min()\n",
    "        all_end = ld.index.max()\n",
    "    \n",
    "        for start in daterange(all_start, all_end, PERIOD):\n",
    "            end = start + timedelta(PERIOD)\n",
    "            period_data = ld.loc[start:end]\n",
    "            \n",
    "            y_columns = set([c for c in df.columns if c.find(\"y_\") > -1])\n",
    "            not_y_columns = [c for c in df.columns if c.find(\"y_\") == -1]\n",
    "            for y_counter in y_columns:\n",
    "                if y_counter not in rdatasX: \n",
    "                    rdatasX[y_counter] = []\n",
    "                    rdatasY[y_counter] = []\n",
    "                    \n",
    "                y_ = period_data[y_counter].to_numpy().astype(float)\n",
    "                y = y_[1:]\n",
    "    \n",
    "                nans = np.count_nonzero(np.isnan(y))\n",
    "    \n",
    "                if float(nans) / y.shape[0] < TRHX and y.shape[0] == PERIOD:\n",
    "                    X = period_data[not_y_columns].to_numpy()[:-1].astype(float)\n",
    "                    X = np.hstack([X,y_[:-1].reshape(-1,1)]) # add current y as a feature\n",
    "                    old_dim = X.shape[1]\n",
    "                    X = imp.fit_transform(X)\n",
    "                    if X.shape[1] == old_dim:\n",
    "                        nans = np.count_nonzero(np.isnan(X))\n",
    "                        if nans == 0:\n",
    "                            y = imp.fit_transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "                            rdatasX[y_counter].append(X)\n",
    "                            rdatasY[y_counter].append(y)\n",
    "                    \n",
    "    all_data.append({k:[np.asarray(rdatasX[k]), np.asarray(rdatasY[k])] for k in rdatasX})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "455aaf36-c9e2-4c5c-a864-c5123484fe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['y_1', 'y_6', 'y_7', 'y_4', 'y_8', 'y_5', 'y_2', 'y_3'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f70c021-80be-439d-9a0e-5f72fe081e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "for k in all_data[0]:\n",
    "    all_data[0][k][0] = normalize(all_data[0][k][0].reshape(-1,all_data[0][k][0].shape[2]),axis=0).reshape(all_data[0][k][0].shape)\n",
    "    \n",
    "for k in all_data[1]:\n",
    "    all_data[1][k][0] = normalize(all_data[1][k][0].reshape(-1,all_data[1][k][0].shape[2]),axis=0).reshape(all_data[1][k][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205d89a-3a6a-4f01-9537-2d70b3cc3018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a59a529a-614a-49e0-884b-0ab0ae3c6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dict_data = {}\n",
    "dict_data[\"Reynolds\"] = {}\n",
    "dict_data[\"UK\"] = {}\n",
    "\n",
    "for k in all_data[0]:\n",
    "    x01,x02,y01,y02 = train_test_split(all_data[0][k][0], all_data[0][k][1], test_size=0.3,random_state=42)\n",
    "    dict_data[\"Reynolds\"][k] = {\"train\":{\"X\":x01,\"y\":y01},\"test\":{\"X\":x02,\"y\":y02}}\n",
    "    \n",
    "for k in all_data[1]:\n",
    "    x11,x12,y11,y12 = train_test_split(all_data[1][k][0], all_data[1][k][1],test_size=0.3,random_state=42)\n",
    "    dict_data[\"UK\"][k] = {\"train\":{\"X\":x11,\"y\":y11},\"test\":{\"X\":x12,\"y\":y12}} \n",
    "\n",
    "all_data = dict_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220b854-2101-4a5b-8d3a-58256166190b",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149bcd0-33fb-4882-bc91-7e69feb34808",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ac0b2-b494-4f11-9966-6988ed981e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==4.21.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd4688-1e45-4fa8-816a-7c0b9a20621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37d7c5-c860-4144-87be-25432e02a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cab2f4-eef5-4c80-a528-bbe0fa4672d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801dd1a7-1dc2-454b-bf0c-afce7e161d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50015bf6-285c-4b21-91e2-cb3c9ebe7a2c",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ee698-ed63-437e-8051-e9152270dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import optuna\n",
    "from tkan import TKAN, BSplineActivation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "nn_data = []\n",
    "\n",
    "\n",
    "def make_modelLSTM(input_shape, hidden_size, dropout):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "    #dim = tf.zeros([batch_size,hidden_size])  \n",
    "    output_layer = tf.keras.layers.LSTM(hidden_size, return_sequences=True,dropout=dropout)(input_layer)#, initial_state=[dim, dim])\n",
    "    output_layer2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear'))(output_layer)    \n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer2)\n",
    " \n",
    "def make_GRU(input_shape, hidden_size, dropout):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "    output_layer = tf.keras.layers.GRU(hidden_size, return_sequences=True,dropout=dropout)(input_layer)#, initial_state=[dim, dim])\n",
    "    output_layer2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear'))(output_layer)    \n",
    "    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "\n",
    "def make_modelTKAN(input_shape, hidden_size, dropout):\n",
    "    model = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "          TKAN(hidden_size, tkan_activations=[BSplineActivation(3)], return_sequences=True, use_bias=True, kernel_regularizer=regularizers.L2(dropout),\n",
    "    bias_regularizer=regularizers.L2(dropout), recurrent_regularizer=regularizers.L2(dropout)),\n",
    "          tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='linear')),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "models = {\"LSTM\":make_modelLSTM, \"GRU\": make_GRU, \"TKAN\": make_modelTKAN}\n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"]\n",
    "            Y_test = dat[\"test\"][\"y\"]\n",
    "            batch_size = 4       \n",
    "    \n",
    "            def objective(trial):\n",
    "                lr = trial.suggest_float('lr', 0.0001, 0.01)\n",
    "                hidden_size = trial.suggest_int('hs', 2, 8)\n",
    "                en = trial.suggest_int('en', 10, 400)\n",
    "\n",
    "                if model_name == \"TKAN\":\n",
    "                    do = trial.suggest_float('dropout', 1e-5, 1e-2)\n",
    "                else:    \n",
    "                    do = trial.suggest_float('dropout', 0.05, 0.2)\n",
    "                    \n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(input_shape=x_train.shape[1:],hidden_size=hidden_size, dropout = do)\n",
    "                # \n",
    "                    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                \n",
    "                    model.compile(\n",
    "                         optimizer=opt,\n",
    "                         loss=\"mean_squared_error\",\n",
    "                         metrics=[\"mean_squared_error\"],\n",
    "                    )\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=en,\n",
    "                         verbose=0,\n",
    "                    )\n",
    "                    try:\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                    except:\n",
    "                        scores.append(500)\n",
    "                return np.asarray(scores).mean() \n",
    "                \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=20)    \n",
    "            \n",
    "            lr = study.best_trial.params[\"lr\"]     \n",
    "            hs = study.best_trial.params[\"hs\"]     \n",
    "            en = study.best_trial.params[\"en\"]   \n",
    "            do = study.best_trial.params[\"dropout\"]   \n",
    "    \n",
    "            model = make_model(input_shape=x_train.shape[1:],hidden_size=hs, dropout = do)\n",
    "        # \n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "            model.compile(\n",
    "                 optimizer=opt,\n",
    "                 loss=\"mean_squared_error\",\n",
    "                 metrics=[\"mean_squared_error\"],\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=en,\n",
    "                 verbose=0,\n",
    "            )        \n",
    "            try:\n",
    "                y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "                mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "                mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "                printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"networks_output.txt\")     \n",
    "                nn_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "            except:\n",
    "                pass\n",
    "            del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cc727-6ec0-4692-9292-dc887ea20006",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0c1e4-e919-4263-a024-313b3fc17b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install deep-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e55aa-3f02-4947-8fb0-0afbd49315e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cb9f5-d07a-40da-a96c-fc6ec26e8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875b738-00c9-466d-acb9-02b3cb7c3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelXGB(max_depth,layers,C):\n",
    "    return xgb.XGBRegressor(max_depth = max_depth, n_estimators = layers)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C):\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4)\n",
    "\n",
    "def make_modelBoosted(max_depth,layers,C):\n",
    "    return CascadeBoostingRegressor(C=C, n_layers=layers, n_estimators = 1, max_depth=max_depth, n_iter_no_change = 1, validation_fraction = 0.1, learning_rate = 0.9)\n",
    "\n",
    "\n",
    "models = {\"XGB\":make_modelXGB,\"Cascade Forest\":make_modelCascade, \"Boosted Forest\": make_modelBoosted}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])\n",
    "            x_test = dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2])\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 1, 2000)\n",
    "                else:\n",
    "                    C = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(max_depth,layers,C)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=20)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]  \n",
    "            else:\n",
    "                C = 0\n",
    "            model = make_model(max_depth,layers,C)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"boosting_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765618c5-d00f-416e-b021-1db9a5bde06d",
   "metadata": {},
   "source": [
    "# Adaptive weighing (AWDF and ECDFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58e59e-ff90-4f6b-8ce6-3838d291645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECDFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1ef4c-4c9a-489c-a42d-72cb00ab22fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-24 14:45:45,504] A new study created in memory with name: no-name-eb115e77-f0af-4ff6-a250-3e15f132b756\n",
      "[I 2024-12-24 14:51:51,914] Trial 0 finished with value: 25.21218899159838 and parameters: {'layers': 12, 'max_depth': 1, 'resampling_rate': 0.10629780388369517, 'et': 0.3376450875821169}. Best is trial 0 with value: 25.21218899159838.\n",
      "[I 2024-12-24 14:58:22,298] Trial 1 finished with value: 8.720234951390589 and parameters: {'layers': 9, 'max_depth': 2, 'resampling_rate': 0.5476335272286831, 'et': 0.5530403227133102}. Best is trial 1 with value: 8.720234951390589.\n",
      "[I 2024-12-24 15:05:22,214] Trial 2 finished with value: 8.649190206872946 and parameters: {'layers': 14, 'max_depth': 2, 'resampling_rate': 1.348378725956127, 'et': 0.505806541981507}. Best is trial 2 with value: 8.649190206872946.\n",
      "[I 2024-12-24 15:09:26,571] Trial 3 finished with value: 26.1301081430162 and parameters: {'layers': 6, 'max_depth': 1, 'resampling_rate': 0.964679126385442, 'et': 0.44409747994336035}. Best is trial 2 with value: 8.649190206872946.\n",
      "[I 2024-12-24 15:12:42,837] Trial 4 finished with value: 25.812431839135304 and parameters: {'layers': 4, 'max_depth': 1, 'resampling_rate': 3.2701792166503023, 'et': 0.3562204032719578}. Best is trial 2 with value: 8.649190206872946.\n",
      "[I 2024-12-24 15:20:34,714] Trial 5 finished with value: 8.034171751109142 and parameters: {'layers': 11, 'max_depth': 2, 'resampling_rate': 2.2630786158307585, 'et': 0.2659509829366463}. Best is trial 5 with value: 8.034171751109142.\n",
      "[I 2024-12-24 15:25:01,499] Trial 6 finished with value: 8.778597659738763 and parameters: {'layers': 6, 'max_depth': 2, 'resampling_rate': 3.555791237750631, 'et': 0.05489854850836784}. Best is trial 5 with value: 8.034171751109142.\n",
      "[I 2024-12-24 15:29:31,945] Trial 7 finished with value: 26.14434783455343 and parameters: {'layers': 10, 'max_depth': 1, 'resampling_rate': 2.9922970187119162, 'et': 0.6762574649220368}. Best is trial 5 with value: 8.034171751109142.\n",
      "[I 2024-12-24 15:31:21,288] Trial 8 finished with value: 26.477098518093396 and parameters: {'layers': 3, 'max_depth': 1, 'resampling_rate': 2.0988243868009975, 'et': 0.4052522014777002}. Best is trial 5 with value: 8.034171751109142.\n",
      "[I 2024-12-24 15:36:17,569] Trial 9 finished with value: 8.423329533902594 and parameters: {'layers': 4, 'max_depth': 2, 'resampling_rate': 1.0727091351107876, 'et': 0.4341603861199891}. Best is trial 5 with value: 8.034171751109142.\n",
      "[I 2024-12-24 15:49:04,370] Trial 10 finished with value: 9.37800002994443 and parameters: {'layers': 15, 'max_depth': 2, 'resampling_rate': 2.4373609965614733, 'et': 0.9215200008056607}. Best is trial 5 with value: 8.034171751109142.\n",
      "[I 2024-12-24 16:00:43,689] Trial 11 finished with value: 8.707585419669515 and parameters: {'layers': 9, 'max_depth': 2, 'resampling_rate': 1.61572296459356, 'et': 0.1938196712532394}. Best is trial 5 with value: 8.034171751109142.\n",
      "[I 2024-12-24 16:04:13,349] Trial 12 finished with value: 7.984039901964636 and parameters: {'layers': 11, 'max_depth': 2, 'resampling_rate': 2.5447337155154597, 'et': 0.19975505910519958}. Best is trial 12 with value: 7.984039901964636.\n",
      "[I 2024-12-24 16:07:41,929] Trial 13 finished with value: 7.930170936821547 and parameters: {'layers': 12, 'max_depth': 2, 'resampling_rate': 2.6259289321965174, 'et': 0.1907567375559317}. Best is trial 13 with value: 7.930170936821547.\n",
      "[I 2024-12-24 16:11:00,553] Trial 14 finished with value: 8.800450184739349 and parameters: {'layers': 13, 'max_depth': 2, 'resampling_rate': 2.7921425338991135, 'et': 0.05772055915693347}. Best is trial 13 with value: 7.930170936821547.\n",
      "[I 2024-12-24 16:14:27,804] Trial 15 finished with value: 8.06806372945749 and parameters: {'layers': 8, 'max_depth': 2, 'resampling_rate': 3.765398695745289, 'et': 0.17391595502479684}. Best is trial 13 with value: 7.930170936821547.\n",
      "[I 2024-12-24 16:20:29,869] Trial 16 finished with value: 9.040542733948811 and parameters: {'layers': 12, 'max_depth': 2, 'resampling_rate': 2.658625878577856, 'et': 0.6520232628063555}. Best is trial 13 with value: 7.930170936821547.\n",
      "[I 2024-12-24 16:32:33,532] Trial 17 finished with value: 8.514846605703342 and parameters: {'layers': 11, 'max_depth': 2, 'resampling_rate': 1.851563442339541, 'et': 0.1819900593037836}. Best is trial 13 with value: 7.930170936821547.\n",
      "[I 2024-12-24 16:41:47,365] Trial 18 finished with value: 7.795866468661321 and parameters: {'layers': 15, 'max_depth': 2, 'resampling_rate': 3.2011548951244158, 'et': 0.2649495240804078}. Best is trial 18 with value: 7.795866468661321.\n",
      "[I 2024-12-24 16:53:04,977] Trial 19 finished with value: 7.796179819075601 and parameters: {'layers': 15, 'max_depth': 2, 'resampling_rate': 3.263389192253206, 'et': 0.2991608020526317}. Best is trial 18 with value: 7.795866468661321.\n",
      "[I 2024-12-24 17:01:30,740] Trial 20 finished with value: 24.51931970251182 and parameters: {'layers': 15, 'max_depth': 1, 'resampling_rate': 3.990562473337773, 'et': 0.2946478194348906}. Best is trial 18 with value: 7.795866468661321.\n",
      "[I 2024-12-24 17:04:56,794] Trial 21 finished with value: 8.22691321084487 and parameters: {'layers': 14, 'max_depth': 2, 'resampling_rate': 3.1929089859271227, 'et': 0.12322381793326821}. Best is trial 18 with value: 7.795866468661321.\n",
      "[I 2024-12-24 17:09:40,057] Trial 22 finished with value: 7.876263327858791 and parameters: {'layers': 13, 'max_depth': 2, 'resampling_rate': 3.384286952462812, 'et': 0.2667245447210681}. Best is trial 18 with value: 7.795866468661321.\n",
      "[I 2024-12-24 17:16:34,528] Trial 23 finished with value: 7.781383447992352 and parameters: {'layers': 14, 'max_depth': 2, 'resampling_rate': 3.4419575411904657, 'et': 0.27967834392113955}. Best is trial 23 with value: 7.781383447992352.\n",
      "[I 2024-12-24 17:25:31,585] Trial 24 finished with value: 7.930865071604562 and parameters: {'layers': 15, 'max_depth': 2, 'resampling_rate': 3.6170063422764933, 'et': 0.34921557764082556}. Best is trial 23 with value: 7.781383447992352.\n",
      "[I 2024-12-24 17:33:01,736] Trial 25 finished with value: 9.379938686898297 and parameters: {'layers': 14, 'max_depth': 2, 'resampling_rate': 2.943814429179201, 'et': 0.9412591363951112}. Best is trial 23 with value: 7.781383447992352.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import optuna\n",
    "\n",
    "from ecdfr.gcForest import gcForest\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelECDFR(max_depth,layers,resampling_rate, et):\n",
    "    config = {\"estimator_configs\":[{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth},{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth},{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth},{\"n_fold\": 5,\"type\":None,\"max_depth\":max_depth}],\n",
    "              \"error_threshold\": et,\n",
    "              \"resampling_rate\": resampling_rate,\n",
    "              \"random_state\":None,\n",
    "              \"max_layers\":layers,\n",
    "              \"early_stop_rounds\":1,\n",
    "              \"train_evaluation\":r2_score}\n",
    "    \n",
    "    return gcForest(config,2)\n",
    "\n",
    "models = {\"ecdfr\":make_modelECDFR}\n",
    "\n",
    "bo_data = []    \n",
    "work_pair = []\n",
    "best_pair = []\n",
    "\n",
    "max_score = 100\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])\n",
    "            x_test = dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2])\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                global max_score\n",
    "                layers = trial.suggest_int('layers', 3, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "\n",
    "                C = trial.suggest_float('resampling_rate', 0.1, 4)\n",
    "                min_et = 0.5 * C - 1\n",
    "                if min_et <= 0:\n",
    "                    min_et = 0.05\n",
    "                else:\n",
    "                    if min_et > 0.99:\n",
    "                        min_et = 0.99\n",
    "\n",
    "                max_et = C -1.\n",
    "\n",
    "                if max_et > 0.99:\n",
    "                    max_et = 0.99\n",
    "                else:\n",
    "                    if max_et <=0:\n",
    "                        max_et = 0.1\n",
    "\n",
    "                if min_et >= max_et:\n",
    "                    max_et = min_et + 0.001\n",
    "                    \n",
    "                et = trial.suggest_float('et', 0.05, 0.95)\n",
    "                \n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                try:\n",
    "                    for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                        model = make_modelECDFR(max_depth,layers,C,et)\n",
    "                    \n",
    "                        model.fit(\n",
    "                             x_train[train_index],\n",
    "                             Y_train[train_index],\n",
    "                        )\n",
    "                        y_pred = model.predict(x_train[test_index]) #, batch_size=batch_size)\n",
    "                        scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                        sc = mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten())\n",
    "                        if max_score == 100:\n",
    "                            max_score = sc\n",
    "                        else:\n",
    "                            if sc > max_score:\n",
    "                                max_score = sc\n",
    "                                \n",
    "                        work_pair.append([C,et])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    scores = [max_score]\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=1000)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "\n",
    "\n",
    "            C = study.best_trial.params[\"resampling_rate\"]  \n",
    "            et = study.best_trial.params[\"et\"]  \n",
    "\n",
    "            best_pair.append([C,et])\n",
    "            model = make_model(max_depth,layers,C,et)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"ecdfr_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832896df-6af8-4b2b-806e-8dbb5c210636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(*zip(*work_pair))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f69de-e579-4cb7-9ec7-1a1d689279ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75081d0-28a8-4abc-9e42-bdd8f859cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028cdcb-1534-4ca0-940e-97f6bd94dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(*zip(*best_pair))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e308d-25b4-44e3-8750-e3d2bfc058f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from boosted_forest import CascadeBoostingRegressor\n",
    "from deepforest import CascadeForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelCascade(max_depth,layers,C,wt):\n",
    "    wf = {0:\"linear\", 1:\"1-w^1/2\", 2:\"1-w2\"}\n",
    "    return CascadeForestRegressor(max_depth = max_depth, max_layers = layers, n_estimators=4,adaptive=True,weighting_function = wf[wt],verbose=0,trx=1.0)\n",
    "\n",
    "\n",
    "models = {\"AWDF\":make_modelCascade}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])\n",
    "            x_test = dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2])\n",
    "            Y_train = dat[\"train\"][\"y\"].flatten()\n",
    "            Y_test = dat[\"test\"][\"y\"].flatten()            \n",
    "\n",
    "            def objective(trial):\n",
    "                layers = trial.suggest_int('layers', 5, 15)\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 2)\n",
    "                wt = trial.suggest_int('weight_function', 0, 2)   \n",
    "                if model_name == \"Boosted Forest\":\n",
    "                    C = trial.suggest_int('C', 1, 2000)\n",
    "                else:\n",
    "                    C = 0\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = make_model(max_depth,layers,C,wt)\n",
    "                    \n",
    "                    model.fit(\n",
    "                         x_train[train_index],\n",
    "                         Y_train[train_index],\n",
    "                    )\n",
    "                    y_pred = model.predict_sampled(x_train[test_index]) #, batch_size=batch_size)\n",
    "                    scores.append(mean_squared_error(Y_train[test_index].flatten(),y_pred.flatten()))\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=500)    \n",
    "            \n",
    "            layers = study.best_trial.params[\"layers\"]  \n",
    "            max_depth = study.best_trial.params[\"max_depth\"]  \n",
    "            wt = study.best_trial.params['weight_function']\n",
    "            if model_name == \"Boosted Forest\":\n",
    "                C = study.best_trial.params[\"C\"]  \n",
    "            else:\n",
    "                C = 0\n",
    "            model = make_model(max_depth,layers,C,wt)\n",
    "            model.fit(\n",
    "                 x_train,\n",
    "                 Y_train,\n",
    "            )        \n",
    "            \n",
    "            y_pred = model.predict_sampled(x_test) #, batch_size=batch_size)\n",
    "            mse_score = mean_squared_error(Y_test.flatten(),y_pred.flatten())\n",
    "            mae_score = mean_absolute_error(Y_test.flatten(),y_pred.flatten())\n",
    "            printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),fname=\"awdf_output.txt\")     \n",
    "            bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869dd8a-6ea3-4538-b81f-0ae41b97d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U numpy==1.22.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffd464-2b3e-4503-84a8-87817cd2e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b679529-47d3-4cef-bf4e-ecd44c7f6bd7",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f10705-60f6-4248-baa1-bc68bdd4e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#xgb.set_config(verbosity=2)\n",
    "\n",
    "def make_modelARIMA(p,d,q,y):\n",
    "    return SARIMAX(y.flatten(), order=(p,d,q),seasonal_order=(1,0,1,7),trend='c')\n",
    "\n",
    "\n",
    "models = {\"ARIMA\": make_modelARIMA}\n",
    "\n",
    "bo_data = []    \n",
    "\n",
    "for model_name in models:\n",
    "    make_model = models[model_name]\n",
    "    for ds_name in all_data:\n",
    "        for depth in all_data[ds_name]:\n",
    "            dat = all_data[ds_name][depth]\n",
    "            x_train = dat[\"train\"][\"X\"]\n",
    "            x_test = dat[\"test\"][\"X\"]\n",
    "            Y_train = dat[\"train\"][\"y\"]\n",
    "            Y_test = dat[\"test\"][\"y\"]\n",
    "\n",
    "            print(Y_train.shape,Y_test.shape)\n",
    "\n",
    "            def objective(trial):\n",
    "                p = trial.suggest_int('p', 1, 5)\n",
    "                d = trial.suggest_int('d', 0, 5)\n",
    "                q = trial.suggest_int('q', 0, 5)\n",
    "\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                try:\n",
    "                    for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                        for y_t in Y_train[train_index]:\n",
    "                            model = make_model(p,d,q, y_t[:27].flatten())\n",
    "                            res = model.fit() \n",
    "                            y_pred = res.forecast(steps = 1) #, batch_size=batch_size)\n",
    "                            \n",
    "                            scores.append(mean_squared_error(np.asarray([y_t[27]]),y_pred))\n",
    "                except:\n",
    "                    scores.append(1000)\n",
    "\n",
    "                return np.asarray(scores).mean() \n",
    "            \n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            study.optimize(objective, n_trials=100)    \n",
    "\n",
    "            try:\n",
    "                p = study.best_trial.params[\"p\"]  \n",
    "                d = study.best_trial.params[\"d\"]  \n",
    "                q = study.best_trial.params[\"q\"]  \n",
    "\n",
    "                mse_list = []\n",
    "                mae_list = []\n",
    "                \n",
    "                for y_t in Y_test:\n",
    "                    model = make_model(p,d,q,y_t[:27])\n",
    "                    res = model.fit() \n",
    "                    y_pred = res.forecast(steps = 1) #, batch_size=batch_size)\n",
    "                    mse_list.append(mean_squared_error(np.asarray([y_t[27]]),y_pred))\n",
    "                    mae_list.append(mean_absolute_error(np.asarray([y_t[27]]),y_pred))\n",
    "                    \n",
    "                mse_score = np.asarray(mse_list).mean()\n",
    "                mae_score = np.asarray(mae_list).mean()\n",
    "                printf(model_name,ds_name,depth,mse_score, mae_score, Y_test.min(),Y_test.max(),\"ARIMA_output.txt\")     \n",
    "                bo_data.append([model_name,ds_name,depth,mse_score, mae_score])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "\n",
    "#print (model.mle_retvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3441aa5-1ed2-4a2d-a015-ce7ba440e584",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517ca83-e93c-430f-b746-3267b2c14792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "rf_data = [] \n",
    "\n",
    "for ds_name in all_data:\n",
    "    for depth in all_data[ds_name]:\n",
    "        best = 1000\n",
    "        best_d = 2\n",
    "        for n_estimators in [100]:\n",
    "            for tdepth in [2,3,5,7,10,12]:\n",
    "                kf = KFold(n_splits=3)\n",
    "                scores = []\n",
    "                for _, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "                    model = RandomForestRegressor(n_estimators = n_estimators, max_depth = tdepth)\n",
    "                    dat = all_data[ds_name][depth]\n",
    "                    model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2])[train_index], dat[\"train\"][\"y\"].flatten()[train_index])\n",
    "                    y_pred = model.predict(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]))[test_index])\n",
    "                    scores.append(mean_squared_error(y_pred, dat[\"train\"][\"y\"].flatten())[test_index])\n",
    "                    \n",
    "                if np.asarray(scores).mean() < best:\n",
    "                    best = np.asarray(scores).mean()\n",
    "                    best_d = tdepth\n",
    "\n",
    "        model = RandomForestRegressor(n_estimators = n_estimators, max_depth = best_d)\n",
    "        dat = all_data[ds_name][depth]\n",
    "        model.fit(dat[\"train\"][\"X\"].reshape(-1,dat[\"train\"][\"X\"].shape[2]), dat[\"train\"][\"y\"].flatten())\n",
    "        y_pred = model.predict(dat[\"test\"][\"X\"].reshape(-1,dat[\"test\"][\"X\"].shape[2]))\n",
    "        mse_score = mean_squared_error(y_pred, dat[\"test\"][\"y\"].flatten())\n",
    "        mae_score = mean_absolute_error(y_pred, dat[\"test\"][\"y\"].flatten())        \n",
    "                \n",
    "        print(\"Random Forest\", ds_name,depth,mse_score, mae_score, dat[\"test\"][\"y\"].min(),dat[\"test\"][\"y\"].max())\n",
    "        rf_data.append([\"Random Forest\", ds_name,depth,mse_score, mae_score])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
